{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk import ngrams\n",
    "#from datasketch.experimental.aio.lsh import AsyncMinHashLSH\n",
    "#from datasketch import MinHash, MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data file couldn't be checked into github because it is too large instead it must be downloaded here #https://www.dropbox.com/s/ir6he8jxxagugnw/assignment3_aricles.json?dl=0datafile =  'data/assignment3_aricles.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_json(\"data/assignment3_aricles.json\", orient='records', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Title</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are you ready to go back to the Moon?  NASA to...</td>\n",
       "      <td>LAMP Educational Site</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WASHINGTON - US Secretary of State Hillary Cli...</td>\n",
       "      <td>Clinton urges Egypt  Israel to talk on Sinai</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The \"hurricane ride-out team\" at NASA's Kenned...</td>\n",
       "      <td>SPACE.com -- Bracing for Impact</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Search the history of over 284 billion web pag...</td>\n",
       "      <td>Internet Archive Wayback Machine</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Islamic State (IS) Sunni radical group ove...</td>\n",
       "      <td>IS overruns parts of Unesco-listed Syrian city</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  \\\n",
       "0  Are you ready to go back to the Moon?  NASA to...   \n",
       "1  WASHINGTON - US Secretary of State Hillary Cli...   \n",
       "2  The \"hurricane ride-out team\" at NASA's Kenned...   \n",
       "3  Search the history of over 284 billion web pag...   \n",
       "4  The Islamic State (IS) Sunni radical group ove...   \n",
       "\n",
       "                                            Title  article_id  \n",
       "0                           LAMP Educational Site           0  \n",
       "1    Clinton urges Egypt  Israel to talk on Sinai           1  \n",
       "2                 SPACE.com -- Bracing for Impact           2  \n",
       "3                Internet Archive Wayback Machine           3  \n",
       "4  IS overruns parts of Unesco-listed Syrian city           4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Title</th>\n",
       "      <th>article_id</th>\n",
       "      <th>ngrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are you ready to go back to the Moon?  NASA to...</td>\n",
       "      <td>LAMP Educational Site</td>\n",
       "      <td>0</td>\n",
       "      <td>('Are', 'you', 'ready', 'to', 'go') ('you', 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WASHINGTON - US Secretary of State Hillary Cli...</td>\n",
       "      <td>Clinton urges Egypt  Israel to talk on Sinai</td>\n",
       "      <td>1</td>\n",
       "      <td>('WASHINGTON', '', 'US', 'Secretary', 'of') ('...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The \"hurricane ride-out team\" at NASA's Kenned...</td>\n",
       "      <td>SPACE.com -- Bracing for Impact</td>\n",
       "      <td>2</td>\n",
       "      <td>('The', 'hurricane', 'rideout', 'team', 'at') ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Search the history of over 284 billion web pag...</td>\n",
       "      <td>Internet Archive Wayback Machine</td>\n",
       "      <td>3</td>\n",
       "      <td>('Search', 'the', 'history', 'of', 'over') ('t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Islamic State (IS) Sunni radical group ove...</td>\n",
       "      <td>IS overruns parts of Unesco-listed Syrian city</td>\n",
       "      <td>4</td>\n",
       "      <td>('The', 'Islamic', 'State', 'IS', 'Sunni') ('I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  \\\n",
       "0  Are you ready to go back to the Moon?  NASA to...   \n",
       "1  WASHINGTON - US Secretary of State Hillary Cli...   \n",
       "2  The \"hurricane ride-out team\" at NASA's Kenned...   \n",
       "3  Search the history of over 284 billion web pag...   \n",
       "4  The Islamic State (IS) Sunni radical group ove...   \n",
       "\n",
       "                                            Title  article_id  \\\n",
       "0                           LAMP Educational Site           0   \n",
       "1    Clinton urges Egypt  Israel to talk on Sinai           1   \n",
       "2                 SPACE.com -- Bracing for Impact           2   \n",
       "3                Internet Archive Wayback Machine           3   \n",
       "4  IS overruns parts of Unesco-listed Syrian city           4   \n",
       "\n",
       "                                              ngrams  \n",
       "0  ('Are', 'you', 'ready', 'to', 'go') ('you', 'r...  \n",
       "1  ('WASHINGTON', '', 'US', 'Secretary', 'of') ('...  \n",
       "2  ('The', 'hurricane', 'rideout', 'team', 'at') ...  \n",
       "3  ('Search', 'the', 'history', 'of', 'over') ('t...  \n",
       "4  ('The', 'Islamic', 'State', 'IS', 'Sunni') ('I...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can use n-gram at word level for this task\n",
    "#try with different n-gram values \n",
    "# You can use ngrams from nltk for this\n",
    "def getNgrams(articles):\n",
    "    n_grams = []\n",
    "    #return articles with a field ngrams\n",
    "    for content in articles['Content']:\n",
    "        n = 5\n",
    "        content = content.split(' ')\n",
    "        content = [x.strip().replace('-', '') for x in content]\n",
    "        content = [x.strip().replace('?', '') for x in content]\n",
    "        content = [x.strip().replace(':', '') for x in content]\n",
    "        content = [x.strip().replace('/', '') for x in content]\n",
    "        content = [x.strip().replace('.', '') for x in content]\n",
    "        content = [x.strip().replace(',', '') for x in content]\n",
    "        content = [x.strip().replace('\\\\', '') for x in content]\n",
    "        content = [x.strip().replace('(', '') for x in content]\n",
    "        content = [x.strip().replace(')', '') for x in content]\n",
    "        content = [x.strip().replace(\"'\", '') for x in content]\n",
    "        content = [x.strip().replace('\"', '') for x in content]\n",
    "        word_grams = []\n",
    "        ngram = ngrams(content,n)\n",
    "        word_grams.append(' '.join(str(i) for i in ngram))\n",
    "        tmp = word_grams[0]\n",
    "        n_grams.append(str(tmp))\n",
    "    articles['temp'] = pd.Series(n_grams) #make new field 'ngrams'. Ngrams are the different sequences of length n in articles.\n",
    "    n_grams = []\n",
    "    #return articles with a field ngrams\n",
    "    for title in articles['Title']:\n",
    "        n = 5\n",
    "        title = title.split(' ')\n",
    "        title = [x.strip().replace('-', '') for x in title]\n",
    "        title = [x.strip().replace('?', '') for x in title]\n",
    "        title = [x.strip().replace(':', '') for x in title]\n",
    "        title = [x.strip().replace('/', '') for x in title]\n",
    "        title = [x.strip().replace('.', '') for x in title]\n",
    "        title = [x.strip().replace(',', '') for x in title]\n",
    "        title = [x.strip().replace('\\\\', '') for x in title]\n",
    "        title = [x.strip().replace('(', '') for x in title]\n",
    "        title = [x.strip().replace(')', '') for x in title]\n",
    "        title = [x.strip().replace(\"'\", '') for x in title]\n",
    "        title = [x.strip().replace('\"', '') for x in title]\n",
    "        word_grams = []\n",
    "        ngram = ngrams(title,n)\n",
    "        word_grams.append(' '.join(str(i) for i in ngram))\n",
    "        tmp2 = word_grams[0]\n",
    "        n_grams.append(str(tmp2))\n",
    "    articles['temp2'] = pd.Series(n_grams) \n",
    "getNgrams(articles)\n",
    "articles['ngrams'] = articles['temp'] + articles['temp2']\n",
    "articles = articles.drop(columns=['temp', 'temp2'])\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert n-grams into binary vector representation for each document. You can do some optimzations if the matrix is too big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def getBinaryMatrix(articles):\n",
    "    #return binary matrix/binary feature vector\n",
    "    #split på ( og ).\n",
    "    #columns = unique n grams\n",
    "    #rows = article ids\n",
    "    #if col in article: 1 else 0\n",
    "    rows = \" \".join(articles.ngrams).split(') (')\n",
    "    \n",
    "    #hvis matrix for stor: bruk n artikler med høyest jaccard similarity.\n",
    "    return rows\n",
    "    \n",
    "rows = getBinaryMatrix(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you ready to go\n"
     ]
    }
   ],
   "source": [
    "for i, s in enumerate(rows):\n",
    "    rows[i] = rows[i].replace(\"(\",'')\n",
    "    rows[i] = rows[i].replace(\")\",'')\n",
    "    rows[i] = rows[i].replace(\"'\",'')\n",
    "    rows[i] = rows[i].replace(',','')\n",
    "\n",
    "\n",
    "print(rows[0]) #a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns = article ids\n",
    "for i in range(rows):\n",
    "    for j in range(articles['Content']):\n",
    "        if rows[i] in articles['Content'][j] || rows[i] in articles['Title'][j]:\n",
    "            data[i,j] = 1\n",
    "        else:\n",
    "            data[i,j] = 0\n",
    "binaryMatrix = pd.DataFrame(data, index=rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need hash function that maps integers 0, 1, . . . , k − 1 to bucket numbers 0 through k − 1. It might be impossible to avoid collisions but as long as the collions are too many it won't matter much.\n",
    "\n",
    "* The simplest would be using the builtin hash() function, it can be for example, hash(rownumber) % Numberofbuckets\n",
    "* You can generate several of these hash functions by xoring a random integer (hash(rownumber)^randint) % Numberofbuckets\n",
    "* It can also be a as simple as (rownumber * randint) % Numberofbuckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHashFunctionValues(numrows, numhashfunctions):\n",
    "    #return a matrix with hash values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute minhash following the faster algorithm from the lecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMinHashSignatureMatrix(binary_matrix, hash_val_matrix)\n",
    "    #return minhash signature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash signature bands into buckets. Find a way to combine all the signature values in a band and hash them into a number of buckets ususally very high.\n",
    "* Easiest way is to add all the signature values in the bucket and use a similar hash function like before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLSH(signature_matrix, hashfunctions, num_bands, num_buckets):\n",
    "   #return lsh buckets or hash table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune parameters to make sure the threshold is appropriate.\n",
    "## plot the probability of two similar items falling in same bucked for different threshold values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the best parameters and get nearest neighbors of each articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the results to submissions.csv file and get the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
