{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'ngrams'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-155e29b6e8ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0marticles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title_And_Content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title_And_Content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"('\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0marticles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title_And_Content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title_And_Content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title_And_Content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title_And_Content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"')\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mngrams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   4374\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4375\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4376\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'ngrams'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "    \n",
    "articles = pd.read_json(\"data/assignment3_aricles.json\", orient='records', encoding=\"utf-8\")\n",
    "articles.head()\n",
    "    \n",
    "for content in articles['Content']:\n",
    "    content = content.split(' ')\n",
    "    content = [x.strip().replace('-', '') for x in content]\n",
    "    content = [x.strip().replace('?', '') for x in content]\n",
    "    content = [x.strip().replace(':', '') for x in content]\n",
    "    content = [x.strip().replace('/', '') for x in content]\n",
    "    content = [x.strip().replace('.', '') for x in content]\n",
    "    content = [x.strip().replace(',', '') for x in content]\n",
    "    content = [x.strip().replace('\\\\', '') for x in content]\n",
    "    content = [x.strip().replace('(', '') for x in content]\n",
    "    content = [x.strip().replace(')', '') for x in content]\n",
    "    content = [x.strip().replace(\"'\", '') for x in content]\n",
    "    content = [x.strip().replace('\"', '') for x in content]\n",
    "    #return articles with a field ngrams\n",
    "for title in articles['Title']:\n",
    "    title = title.split(' ')\n",
    "    title = [x.strip().replace('-', '') for x in title]\n",
    "    title = [x.strip().replace('?', '') for x in title]\n",
    "    title = [x.strip().replace(':', '') for x in title]\n",
    "    title = [x.strip().replace('/', '') for x in title]\n",
    "    title = [x.strip().replace('.', '') for x in title]\n",
    "    title = [x.strip().replace(',', '') for x in title]\n",
    "    title = [x.strip().replace('\\\\', '') for x in title]\n",
    "    title = [x.strip().replace('(', '') for x in title]\n",
    "    title = [x.strip().replace(')', '') for x in title]\n",
    "    title = [x.strip().replace(\"'\", '') for x in title]\n",
    "    title = [x.strip().replace('\"', '') for x in title]\n",
    "articles['Title_And_Content'] = articles['Title'] + articles['Content']\n",
    "articles.head()\n",
    "    \n",
    "articles['Title_And_Content'] = articles['Title_And_Content'].str.lower()\n",
    "articles['Title_And_Content'] = articles['Title_And_Content'].str.replace(\"', '\",' ').str.split(\"'\\) \\('\")\n",
    "for i,x in enumerate(articles['Title_And_Content']):\n",
    "    articles['Title_And_Content'][i][0] = articles['Title_And_Content'][i][0].replace(\"('\",'')\n",
    "    articles['Title_And_Content'][i][len(articles['Title_And_Content'][i])-1] = articles['Title_And_Content'][i][len(articles['Title_And_Content'][i])-1].replace(\"')\",'')\n",
    "print(articles['Title_And_Content'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48505\n"
     ]
    }
   ],
   "source": [
    "print(articles.index.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fnv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-382e951f7039>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfnv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0muuid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fnv'"
     ]
    }
   ],
   "source": [
    "#see gorina.rtf\n",
    "from fnv import *\n",
    "import uuid\n",
    "import numpy as np\n",
    "\n",
    "threshold = 0.6\n",
    "s = 14  # num hash tables\n",
    "r = 6   # minhash signature size\n",
    "p = 15373875993579943603 # 64-bit prime num\n",
    "\n",
    "def main(): \n",
    "    questions = {}\n",
    "    for index in articles.index:\n",
    "        questions[index] = articles['Title_And_Content'][index]\n",
    "    numquestions = articles.index.size\n",
    "    # creating indexable random numbers for hashing\n",
    "    seta = []\n",
    "    setb = []\n",
    "    for i in range(r):\n",
    "        seta.append(uuid.uuid4().int & (1<<64)-1)\n",
    "        setb.append(uuid.uuid4().int & (1<<64)-1)\n",
    "    \n",
    "    # build dataset of hashtables\n",
    "    Dataset = []\n",
    "    # set of all minHashes\n",
    "    minHashes = np.empty((s,numquestions,r))    \n",
    "\n",
    "\n",
    "    # create s hashtables\n",
    "    for i in range(s):\n",
    "        hashtable = {}  \n",
    "        k = 0\n",
    "        for qid, words in questions.items():\n",
    "            minSig = np.empty(r)\n",
    "            # loops through all hash functions\n",
    "            for j in range(r):\n",
    "                minHash = p+1\n",
    "                # hashes each word\n",
    "                for word in words:\n",
    "                    hc = hashFunc(word, seta[j], setb[j])\n",
    "                    if hc < minHash:\n",
    "                        minHash = hc\n",
    "                # stores the minimum hash\n",
    "                minHashes[i,k,j] = minHash\n",
    "            # stores the qid indexed by the minHash sequence into the current hashtable\n",
    "            temp = ''.join(str(x) for x in minHashes[i,k])\n",
    "            if temp not in hashtable:\n",
    "                hashtable[temp] = [qid] * 1\n",
    "            elif qid not in hashtable[temp]:\n",
    "                hashtable[temp].append(qid)\n",
    "            k += 1\n",
    "        Dataset.append(hashtable)\n",
    "\n",
    "    # prints headers, checks each hashtable indexed by each items minHash\n",
    "    # for similar items (by locality sensitivity), and outputs them next to each respective qid\n",
    "    print()\n",
    "    print(\"qid\\tsimilar-qids\")\n",
    "    k = 0\n",
    "    nearest_neighbors=[]\n",
    "    articleIDs = []\n",
    "    for qid, words in questions.items():\n",
    "        similarSet = []\n",
    "        for i in range(s):\n",
    "            hashtable = Dataset[i]\n",
    "\n",
    "            # retrieve the minHash sequence for this item in the current hashtable\n",
    "            for newqid in hashtable[''.join(str(x) for x in minHashes[i,k])]:\n",
    "                if newqid != qid:                                           # exclude the current item\n",
    "                    if newqid not in similarSet:                            # dont add dups\n",
    "                        if findSims(questions[qid], questions[newqid]):     # use Native Jaccard to remove *FALSE POSITIVES*\n",
    "                            similarSet.append(newqid)\n",
    "        k += 1\n",
    "        print(\"%s\\t\" %qid, end='')\n",
    "        print(','.join(similarSet))\n",
    "        articleIDs.append(qid)\n",
    "        nearest_neighbours.append(','.join(similarSet))\n",
    "    sub = pd.DataFrame()\n",
    "    sub['Id'] = articleIDs #article IDs\n",
    "    sub['NearestNeighbours'] = nearest_neighbours\n",
    "    sub.to_csv('submission.csv',index=False)\n",
    "    \n",
    "        \n",
    "# encodes the word using fnv, and returns a basic hashed \n",
    "# value using random ints a,b and a fixed prime number p\n",
    "def hashFunc(word,a,b):\n",
    "    x = hash(word.encode('utf-8'), bits=64)\n",
    "    hc = (a*x+b) % p\n",
    "    return hc\n",
    "\n",
    "def findSims(words1, words2):\n",
    "\n",
    "    # Naive Jaccard Algorithm\n",
    "    # loops through all words and divides intersect of words1 and words2 by the union\n",
    "    # and adds the qid to similarqid[] if the answer > threshold\n",
    "    intersect = 0\n",
    "    union = len(words1)\n",
    "    for word in words2:\n",
    "        if word in words1:\n",
    "            intersect += 1\n",
    "            continue\n",
    "        union += 1\n",
    "    if (intersect / union) >= threshold:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
