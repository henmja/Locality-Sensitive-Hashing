{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "import json\n",
    "from nltk import ngrams\n",
    "#from  datasketch.experimental.aio.lsh import AsyncMinHashLSH\n",
    "#from  datasketch import MinHash, MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_json(\"data/assignment3_aricles.json\", orient='records', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Title</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are you ready to go back to the Moon?  NASA to...</td>\n",
       "      <td>LAMP Educational Site</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WASHINGTON - US Secretary of State Hillary Cli...</td>\n",
       "      <td>Clinton urges Egypt  Israel to talk on Sinai</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The \"hurricane ride-out team\" at NASA's Kenned...</td>\n",
       "      <td>SPACE.com -- Bracing for Impact</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Search the history of over 284 billion web pag...</td>\n",
       "      <td>Internet Archive Wayback Machine</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Islamic State (IS) Sunni radical group ove...</td>\n",
       "      <td>IS overruns parts of Unesco-listed Syrian city</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  \\\n",
       "0  Are you ready to go back to the Moon?  NASA to...   \n",
       "1  WASHINGTON - US Secretary of State Hillary Cli...   \n",
       "2  The \"hurricane ride-out team\" at NASA's Kenned...   \n",
       "3  Search the history of over 284 billion web pag...   \n",
       "4  The Islamic State (IS) Sunni radical group ove...   \n",
       "\n",
       "                                            Title  article_id  \n",
       "0                           LAMP Educational Site           0  \n",
       "1    Clinton urges Egypt  Israel to talk on Sinai           1  \n",
       "2                 SPACE.com -- Bracing for Impact           2  \n",
       "3                Internet Archive Wayback Machine           3  \n",
       "4  IS overruns parts of Unesco-listed Syrian city           4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Title</th>\n",
       "      <th>article_id</th>\n",
       "      <th>ngrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are you ready to go back to the Moon?  NASA to...</td>\n",
       "      <td>LAMP Educational Site</td>\n",
       "      <td>0</td>\n",
       "      <td>('Are', 'you') ('you', 'ready') ('ready', 'to'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WASHINGTON - US Secretary of State Hillary Cli...</td>\n",
       "      <td>Clinton urges Egypt  Israel to talk on Sinai</td>\n",
       "      <td>1</td>\n",
       "      <td>('WASHINGTON', '') ('', 'US') ('US', 'Secretar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The \"hurricane ride-out team\" at NASA's Kenned...</td>\n",
       "      <td>SPACE.com -- Bracing for Impact</td>\n",
       "      <td>2</td>\n",
       "      <td>('The', 'hurricane') ('hurricane', 'rideout') ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Search the history of over 284 billion web pag...</td>\n",
       "      <td>Internet Archive Wayback Machine</td>\n",
       "      <td>3</td>\n",
       "      <td>('Search', 'the') ('the', 'history') ('history...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Islamic State (IS) Sunni radical group ove...</td>\n",
       "      <td>IS overruns parts of Unesco-listed Syrian city</td>\n",
       "      <td>4</td>\n",
       "      <td>('The', 'Islamic') ('Islamic', 'State') ('Stat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  \\\n",
       "0  Are you ready to go back to the Moon?  NASA to...   \n",
       "1  WASHINGTON - US Secretary of State Hillary Cli...   \n",
       "2  The \"hurricane ride-out team\" at NASA's Kenned...   \n",
       "3  Search the history of over 284 billion web pag...   \n",
       "4  The Islamic State (IS) Sunni radical group ove...   \n",
       "\n",
       "                                            Title  article_id  \\\n",
       "0                           LAMP Educational Site           0   \n",
       "1    Clinton urges Egypt  Israel to talk on Sinai           1   \n",
       "2                 SPACE.com -- Bracing for Impact           2   \n",
       "3                Internet Archive Wayback Machine           3   \n",
       "4  IS overruns parts of Unesco-listed Syrian city           4   \n",
       "\n",
       "                                              ngrams  \n",
       "0  ('Are', 'you') ('you', 'ready') ('ready', 'to'...  \n",
       "1  ('WASHINGTON', '') ('', 'US') ('US', 'Secretar...  \n",
       "2  ('The', 'hurricane') ('hurricane', 'rideout') ...  \n",
       "3  ('Search', 'the') ('the', 'history') ('history...  \n",
       "4  ('The', 'Islamic') ('Islamic', 'State') ('Stat...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can use n-gram at word level for this task\n",
    "#try with different n-gram values \n",
    "# You can use ngrams from nltk for this\n",
    "def getNgrams(articles):\n",
    "    n_grams = []\n",
    "    #return articles with a field ngrams\n",
    "    #articles['Content'] = articles['Content'].map(lambda x: x.lstrip(\"')(-?:/.,\").rstrip('aAbBcC'))\n",
    "    for content in articles['Content']:\n",
    "        n = 2\n",
    "        content = content.split(' ')\n",
    "        content = [x.strip().replace('-', '') for x in content]\n",
    "        content = [x.strip().replace('?', '') for x in content]\n",
    "        content = [x.strip().replace(':', '') for x in content]\n",
    "        content = [x.strip().replace('/', '') for x in content]\n",
    "        content = [x.strip().replace('.', '') for x in content]\n",
    "        content = [x.strip().replace(',', '') for x in content]\n",
    "        content = [x.strip().replace('\\\\', '') for x in content]\n",
    "        content = [x.strip().replace('(', '') for x in content]\n",
    "        content = [x.strip().replace(')', '') for x in content]\n",
    "        content = [x.strip().replace(\"'\", '') for x in content]\n",
    "        content = [x.strip().replace('\"', '') for x in content]\n",
    "        word_grams = []\n",
    "        ngram = ngrams(content,n)\n",
    "        word_grams.append(' '.join(str(i) for i in ngram))\n",
    "        tmp = word_grams[0]\n",
    "        n_grams.append(str(tmp))\n",
    "    articles['temp'] = pd.Series(n_grams) #make new field 'ngrams'. Ngrams are the different sequences of length n in articles.\n",
    "    n_grams = []\n",
    "    #return articles with a field ngrams\n",
    "    for title in articles['Title']:\n",
    "        n = 2\n",
    "        title = title.split(' ')\n",
    "        title = [x.strip().replace('-', '') for x in title]\n",
    "        title = [x.strip().replace('?', '') for x in title]\n",
    "        title = [x.strip().replace(':', '') for x in title]\n",
    "        title = [x.strip().replace('/', '') for x in title]\n",
    "        title = [x.strip().replace('.', '') for x in title]\n",
    "        title = [x.strip().replace(',', '') for x in title]\n",
    "        title = [x.strip().replace('\\\\', '') for x in title]\n",
    "        title = [x.strip().replace('(', '') for x in title]\n",
    "        title = [x.strip().replace(')', '') for x in title]\n",
    "        title = [x.strip().replace(\"'\", '') for x in title]\n",
    "        title = [x.strip().replace('\"', '') for x in title]\n",
    "        word_grams = []\n",
    "        ngram = ngrams(title,n)\n",
    "        word_grams.append(' '.join(str(i) for i in ngram))\n",
    "        tmp2 = word_grams[0]\n",
    "        n_grams.append(str(tmp2))\n",
    "    articles['temp2'] = pd.Series(n_grams) \n",
    "getNgrams(articles)\n",
    "articles['ngrams'] = articles['temp'] + articles['temp2']\n",
    "articles = articles.drop(columns=['temp', 'temp2'])\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert n-grams into binary vector representation for each document. You can do some optimzations if the matrix is too big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are you\n"
     ]
    }
   ],
   "source": [
    "articles['ngrams'] = articles['ngrams'].str.lower()\n",
    "articles['ngrams'] = articles['ngrams'].str.replace(\"', '\",' ').str.split(\"'\\) \\('\")\n",
    "for i,x in enumerate(articles['ngrams']):\n",
    "    articles['ngrams'][i][0] = articles['ngrams'][i][0].replace(\"('\",'')\n",
    "    articles['ngrams'][i][len(articles['ngrams'][i])-1] = articles['ngrams'][i][len(articles['ngrams'][i])-1].replace(\"')\",'')\n",
    "print(articles.ngrams[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34565, 3735880)\n"
     ]
    }
   ],
   "source": [
    "from  scipy.sparse import csr_matrix\n",
    "\n",
    "docs = articles.index\n",
    "indptr = [0] \n",
    "indices = [] \n",
    "data = [] \n",
    "vocabulary = {} \n",
    "for d in docs:\n",
    "    for ngram in articles.ngrams[d]:\n",
    "        index = vocabulary.setdefault(ngram, len(vocabulary)) \n",
    "        indices.append(index) \n",
    "        data.append(1) \n",
    "    indptr.append(len(indices))\n",
    "binaryMatrix = csr_matrix((data, indices, indptr), dtype=int)\n",
    "print(binaryMatrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3735880, 34565)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "binaryMatrixTransposed = binaryMatrix.transpose()\n",
    "print(binaryMatrixTransposed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need hash function that maps integers 0, 1, . . . , k − 1 to bucket numbers 0 through k − 1. It might be impossible to avoid collisions but as long as the collions are too many it won't matter much.\n",
    "\n",
    "* The simplest would be using the builtin hash() function, it can be for example, hash(rownumber) % Numberofbuckets\n",
    "* You can generate several of these hash functions by xoring a random integer (hash(rownumber)^randint) % Numberofbuckets\n",
    "* It can also be a as simple as (rownumber * randint) % Numberofbuckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def getHashFunctionValues(numrows, numhashfunctions):\n",
    "    #return a matrix with hash values \n",
    "    numBuckets = numrows \n",
    "    hashValMatrix = np.empty((numrows, numhashfunctions)) \n",
    "    rand_int = [random.randint(1, numrows) for i in range(0,numhashfunctions)]\n",
    "    for i in range(0,numrows):\n",
    "        hashValMatrix[i,:] = [(i^rand_int[j]) % numBuckets for j in range(numhashfunctions)] \n",
    "    return hashValMatrix\n",
    "\n",
    "hashValMatrix=getHashFunctionValues(numrows=binaryMatrixTransposed.shape[0], numhashfunctions=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3735880, 100)\n"
     ]
    }
   ],
   "source": [
    "print(hash_val_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute minhash following the faster algorithm from the lecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMinHashSignatureMatrix(binary_matrix, hash_val_matrix)\n",
    "    #return minhash signature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash signature bands into buckets. Find a way to combine all the signature values in a band and hash them into a number of buckets ususally very high.\n",
    "* Easiest way is to add all the signature values in the bucket and use a similar hash function like before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLSH(signature_matrix, hashfunctions, num_bands, num_buckets):\n",
    "   #return lsh buckets or hash table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune parameters to make sure the threshold is appropriate.\n",
    "## plot the probability of two similar items falling in same bucked for different threshold values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the best parameters and get nearest neighbors of each articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the results to submissions.csv file and get the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write article ids, nearest neighbours (documents in same buckets)\n",
    "\n",
    "#DUE 18.03"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
