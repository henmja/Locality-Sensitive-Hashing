{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Building a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a skeleton of a decision tree classifier for the example data set in `data/example.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "from statistics import median, mode, mean\n",
    "from collections import Counter\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import scipy.stats as st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input filename is hard-coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet = np.loadtxt('data/housing_price_train.csv',dtype='<U20',delimiter=',')\n",
    "testSet = np.loadtxt('data/housing_price_test.csv',dtype='<U20',delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute labels types are hard-coded too (the same order as in the file!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_attributes = testSet[0]\n",
    "trainingTarget= np.transpose(np.copy(trainingSet[1:]))\n",
    "testData = np.transpose(np.copy(testSet[1:]))\n",
    "testData = np.copy(testData)\n",
    "test_attr_names =np.copy(test_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index of the target attribute (assuming it's the last)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_TARGET = np.log10(np.float64(trainingTarget[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A main class DT representing the decision tree classifier. It could represent with methods:\n",
    "\n",
    "  - a given impurity measure;\n",
    "  - the search for the best attribute to split with;\n",
    "  - the addition of a node to the tree;\n",
    "  - a convenient model printer;\n",
    "  - the recursive call for obtaining a tree;\n",
    "  - a builder and an applier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henri\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:185: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Gain: 8.045796138342293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henri\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:215: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "C:\\Users\\henri\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:216: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Gain: 7.446101489756081\n",
      "Max Gain: 6.524561605715065\n",
      "Max Gain: 6.497662618424865\n",
      "Max Gain: 5.65645677569764\n",
      "Max Gain: 5.631615665225583\n",
      "Max Gain: 5.8574319218048085\n",
      "Max Gain: 6.595891163108825\n",
      "Max Gain: 7.149981774453494\n",
      "Max Gain: 7.43752996644357\n",
      "Max Gain: 7.331770310806017\n",
      "Max Gain: 7.21647843135994\n",
      "Max Gain: 7.813184524595746\n",
      "Max Gain: 7.126460214904276\n",
      "Max Gain: 6.247495789386386\n",
      "Max Gain: 5.94770277922009\n",
      "Max Gain: 6.058984089445426\n",
      "Max Gain: 6.87496155606346\n",
      "Max Gain: 7.164965687925738\n",
      "Max Gain: 6.457972024550777\n",
      "Max Gain: 6.11249200111032\n",
      "Max Gain: 5.304682449772211\n",
      "Max Gain: 4.566108939837479\n",
      "Max Gain: 4.436605434317882\n",
      "Max Gain: 3.321928094887362\n",
      "Max Gain: 3.7004397181410926\n",
      "Max Gain: 6.364489555653822\n",
      "Max Gain: 5.720049960644811\n",
      "Max Gain: 5.155399311574898\n"
     ]
    }
   ],
   "source": [
    "def stringToInt(attribute):\n",
    "    ''' \n",
    "    Convert numeric attributes from string to int\n",
    "    '''\n",
    "    uniq_vals, val_counts = np.unique(attribute,return_counts=True)\n",
    "    map_ints = range(len(uniq_vals))\n",
    "    map_attr = np.copy(attribute)\n",
    "    for i in range(len(uniq_vals)):\n",
    "        all_val = np.where(attribute == uniq_vals[i])[0]\n",
    "        map_attr[all_val] = map_ints[i]\n",
    "        intvals = map_ints[i]\n",
    "        allints = np.ones_like(all_val)*intvals\n",
    "    return [uniq_vals, map_ints, val_counts,map_attr]\n",
    "def probabilities(attribute):\n",
    "    ''' \n",
    "    Probabilities for entropy calculation\n",
    "    '''\n",
    "    uniq_vals, val_counts=np.unique(attribute,return_counts=True)\n",
    "    val_counts = np.float64(val_counts)\n",
    "    val_probs = val_counts/np.float64(attribute.size)\n",
    "    return uniq_vals, val_probs\n",
    "def entropy(attribute):\n",
    "    ''' \n",
    "    Compute entropy, which is used to compute infogain\n",
    "    '''\n",
    "    uniq_vals , val_probs =probabilities(attribute)\n",
    "    return np.sum(val_probs *np.log2(1./val_probs))\n",
    "def infogain(attribute,labels):\n",
    "    ''' \n",
    "    Compute infogains\n",
    "    '''\n",
    "    cts_arr =probabilities(attribute)\n",
    "    label_entropy = entropy(labels)\n",
    "    label_given_attr = []\n",
    "    if type(cts_arr[1]) != np.ndarray:\n",
    "        return label_entropy\n",
    "    cts_dict = dict(zip(cts_arr[0],cts_arr[1]))\n",
    "    for uniq_attr_val,uniq_val_prob in cts_dict.items():\n",
    "        label_given_attr.append( uniq_val_prob *entropy(labels[attribute==uniq_attr_val]))\n",
    "    return label_entropy- np.sum(label_given_attr)\n",
    "def bestAttr(data, attribute_names,labels):\n",
    "    ''' \n",
    "    the best attribute is the one that gives maximum infogain\n",
    "    '''\n",
    "    gains=np.array([])\n",
    "    for i in data:\n",
    "        gains=np.append( gains, infogain(i, labels))\n",
    "    maxind = np.where(gains == np.max(gains))[0][0]\n",
    "    maxgn = gains[maxind]\n",
    "    att_name= attribute_names[maxind]\n",
    "    print('Max Gain:', maxgn)\n",
    "    return att_name,maxind\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DT:\n",
    "    def __init__(self, attr_split=None,parent=None):\n",
    "        self.attr_split= attr_split\n",
    "        self.parent=parent\n",
    "        self.child=[]\n",
    "        \n",
    "    def add_child(self,child):\n",
    "        ''' \n",
    "        Add child node\n",
    "        '''\n",
    "        self.child.append(child)\n",
    "        child.parent= self\n",
    "        \n",
    "    def __mean_squared_error(self, records):\n",
    "        \"\"\"\n",
    "        Calculates mean squared error for a selection of records.\n",
    "\n",
    "        :param records: Data records (given by indices)\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        predictions = self.predict(records)\n",
    "        sqe = np.zeros(len(records)) #square error\n",
    "        i = 0\n",
    "        for rec in records:\n",
    "            sqe[i] = (self.data[records,IDX_TARGET]-predictions[records, IDX_TARGET])**2 \n",
    "            i = i+1\n",
    "        MSE = mean(sqe) #mean square error\n",
    "        return MSE\n",
    "    \n",
    "    \n",
    "def pureAttr(vals):\n",
    "    ''' \n",
    "    Attribute with a purity of 1\n",
    "    '''\n",
    "    return len(np.unique(vals))==1\n",
    "\n",
    "def isFinite(arr1,arr2):\n",
    "    ''' \n",
    "    Check if input arrays consist of finite numbers\n",
    "    '''\n",
    "    finds = np.where((~np.isnan(arr1)) &(~np.isnan(arr2)))[0]\n",
    "    fin_arr1 = arr1[finds]\n",
    "    fin_arr2 = arr2[finds]\n",
    "    return fin_arr1,fin_arr2,finds\n",
    "\n",
    "def split(attr,labels,attr_name,depth=0):\n",
    "    ''' \n",
    "    Split attribute data\n",
    "    '''\n",
    "    try:\n",
    "        attr_dat =np.float64(attr)\n",
    "        uniq_vals,probs = probabilities(attr)\n",
    "        typ='num'    \n",
    "    except:\n",
    "        uniq_vals,map_ints, val_counts, map_attr = stringToInt(attr)\n",
    "        attr_dat = np.copy(np.float64(map_attr))\n",
    "        typ='nom'\n",
    "        print('Map Attr',map_attr)\n",
    "    if typ =='nom' or len(uniq_vals) <=10:\n",
    "        n_splits = len(uniq_vals)\n",
    "        splits =[]\n",
    "        split_ints = []\n",
    "        for i in range(n_splits):\n",
    "            splits.append([attr==uniq_vals[i] ])\n",
    "            split_ints.append(uniq_vals[i])\n",
    "        threshold_val = np.array([-99])\n",
    "        threshold_type ='NA'\n",
    "    elif len(uniq_vals) >2:\n",
    "        try:\n",
    "            labels = np.float64(labels)\n",
    "            sort_by_attr = np.argsort(attr_dat)\n",
    "            attr_sort = np.copy(attr_dat[sort_by_attr])\n",
    "            label_sort =np.copy(labels[sort_by_attr])\n",
    "            linearRegslopes_left = []\n",
    "            linearRegslopes_right = []\n",
    "            for i in range(len(attr_sort)//10,len(attr_sort) -len(attr_sort)//10  ):\n",
    "                attr_l = attr_sort[0:i]\n",
    "                attr_r = attr_sort[i:]\n",
    "                lab_l = label_sort[0:i]\n",
    "                lab_r = label_sort[i:]\n",
    "                m_l,b_l = np.polyfit(attr_l,lab_l,1)\n",
    "                m_r,b_r = np.polyfit(attr_r,lab_r,1)\n",
    "                linearRegslopes_left.append(abs(m_l))\n",
    "                linearRegslopes_right.append(abs(m_r))\n",
    "            linearRegslopes_left =np.array(linearRegslopes_left)\n",
    "            linearRegslopes_right =np.array(linearRegslopes_right)\n",
    "            finite_left,finite_right,finite_inds = is_finite(linearRegslopes_left,linearRegslopes_right)\n",
    "            inv_left = 1./finite_left\n",
    "            inv_right = 1./finite_right\n",
    "            linearLeft=finite_left*inv_right\n",
    "            linearRight = inv_left*finite_right\n",
    "            finmxleft = np.where(linearLeft == np.max(linearLeft))[0][0]\n",
    "            mxleft = np.where(linearRegslopes_left == finite_left[finmxleft])[0]\n",
    "            finmxright = np.where(linearRight == np.max(linearRight))[0][0]\n",
    "            mxright = np.where(linearRegslopes_right == finite_right[finmxright])[0]\n",
    "            threshold_val = attr_sort[mxleft+len(attr_sort)//10]\n",
    "                   \n",
    "            if typ=='nom':\n",
    "                split_l = [attr_dat <threshold_val]\n",
    "                split_r = [attr_dat >=threshold_val]\n",
    "                splits=[split_l,split_r]\n",
    "            else:\n",
    "                split_l = [attr_dat<=threshold_val]\n",
    "                split_r= [attr_dat >threshold_val]\n",
    "                splits=[split_l,split_r]\n",
    "            threshold_type='linsplit'\n",
    "            split_ints =[]\n",
    "        except:\n",
    "            threshold_val = np.array([np.mean(attr_dat)])\n",
    "            split_l =[attr_dat < threshold_val]\n",
    "            split_r = [attr_dat >= threshold_val]\n",
    "            splits = [split_l,split_r]\n",
    "            threshold_type = 'linsplit'\n",
    "            split_ints = []\n",
    "    return splits,threshold_val,threshold_type,split_ints,typ\n",
    "\n",
    "def print_tree(tree):\n",
    "    '''\n",
    "    Print split info\n",
    "    '''\n",
    "    print(tree.split_attr)\n",
    "    print(tree.threshold_val)\n",
    "    print(tree.avg_label)\n",
    "\n",
    "def buildTree(dat,attribute_names,labels,root=None,depth=0):\n",
    "    '''\n",
    "    Find best attribute to split on, the values for each splits and build a decision tree by calling this function recursively.\n",
    "    '''\n",
    "    if dat[0] ==[]:\n",
    "        root.avg_label = root.parent.avg_label\n",
    "        return \n",
    "    if len(dat[0]) <5:\n",
    "        root.avg_label= np.mean(labels)\n",
    "        return\n",
    "    bestattr,maxind= bestAttr(dat,attribute_names,labels)\n",
    "    if not root:\n",
    "        root = DT()\n",
    "    root.split_attr =bestattr\n",
    "    root.split_attr_ind=maxind\n",
    "    attr_data = np.copy(dat[maxind])\n",
    "    if pureAttr(attr_data):\n",
    "        root.avg_label= np.mean(labels)\n",
    "        return \n",
    "    splits,threshold_val, threshold_type,split_ints,typ=split(attr_data,labels,bestattr,depth=depth)\n",
    "    trees = []\n",
    "    for i in range(len(splits)):\n",
    "        trees.append(Tree(parent=root))\n",
    "        root.add_child(trees[i])\n",
    "        root.split_ints = split_ints\n",
    "        trees[i].inds_filt = splits[i][0]\n",
    "        \n",
    "    root.threshold_val =threshold_val\n",
    "    root.threshold_type = threshold_type\n",
    "    root.typ = typ\n",
    "    root.avg_label= np.mean(labels)\n",
    "    depth+=1\n",
    "    red= [attribute_names!=bestattr]\n",
    "\n",
    "    red_attr_names = attribute_names[red]\n",
    "    red_data = dat[red]\n",
    "    for tree in trees:\n",
    "        buildTree(red_data[:,tree.inds_filt],red_attr_names,labels[tree.inds_filt],root=tree,depth=depth)\n",
    "    return root \n",
    "targetId = {}\n",
    "tree = buildTree(data,attr_names, IDX_TARGET)\n",
    "\n",
    "\n",
    "def predictlabels(dat,attr_names,dTree,predTree = None,labels=[]):\n",
    "    '''\n",
    "    Predict target/class values\n",
    "    '''\n",
    "    if len(dat[0]) == 0:\n",
    "        return\n",
    "    if len(labels) == 0:\n",
    "        labels = np.ones_like(dat[0])\n",
    "    if len(dTree.child) ==0:\n",
    "        for tid in dat[0]:\n",
    "            targetId[tid] = dTree.avg_label \n",
    "        return\n",
    "    if not predTree:\n",
    "        predTree = Tree()\n",
    "    attr_split = dTree.split_attr \n",
    "    attr_split_ind =np.where(attr_names == attr_split)[0]\n",
    "    attr_split_ints = dTree.split_ints\n",
    "    attr_dat = dat[attr_split_ind][0] \n",
    "    thresh_type = dTree.threshold_type\n",
    "    thresh_val = np.float64(dTree.threshold_val[0])\n",
    "    try:\n",
    "        attr_data =np.float64(attr_dat)\n",
    "        uniq_vals,probs = probabilities(attr_dat)\n",
    "    except:\n",
    "        uniq_vals,map_ints, val_counts, map_attr = stringToInt(attr_dat)\n",
    "        attr_data = np.copy(np.float64(map_attr))\n",
    "    typ = dTree.typ\n",
    "    predTree.attr_split = attr_split\n",
    "    if thresh_type=='0':\n",
    "        split_l = np.where(attr_data == 0)[0]\n",
    "        split_r = np.where(attr_data > 0)[0]\n",
    "        splits = [split_l,split_r]\n",
    "    elif thresh_type=='linsplit':\n",
    "        if typ=='nom':\n",
    "            split_l = np.where(attr_data <thresh_val)[0]\n",
    "            split_r = np.where(attr_data >=thresh_val)[0]\n",
    "            splits=[split_l,split_r]\n",
    "        else:\n",
    "            split_l = np.where(attr_data<=thresh_val)[0]\n",
    "            split_r= np.where(attr_data >thresh_val)[0]       \n",
    "            splits=[split_l,split_r]   \n",
    "    elif thresh_type == 'NA':\n",
    "        splits =[]\n",
    "        for val in attr_split_ints:\n",
    "            splits.append(np.where(attr_data==val)[0])    \n",
    "    trees = []\n",
    "    for i in range(len(splits)):\n",
    "        trees.append(Tree(parent=predTree))\n",
    "        predTree.add_child(trees[i])\n",
    "        trees[i].label_inds = splits[i]\n",
    "        trees[i].dtree = dTree.child[i]\n",
    "        trees[i].avg_label =dTree.avg_label\n",
    "        predictlabels(dat[:,trees[i].label_inds],attr_names,trees[i].dtree, predTree= trees[i],labels=labels)\n",
    "    tids = dat[0]\n",
    "    labs =[]\n",
    "    for tid in tids:\n",
    "        try:\n",
    "            labs.append(targetId[tid])\n",
    "        except:\n",
    "            labs.append(np.median(list(targetId.values() )))\n",
    "    return labs\n",
    "p = predictlabels(testData, test_attr_names,tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def createSubmission(test_ids, predictions):\n",
    "    sub = pd.DataFrame()\n",
    "    sub['Id'] = test_ids\n",
    "    sub['SalePrice'] = predictions\n",
    "    sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the main function building a decision tree model, printing it and applying it on some unseen records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LotArea\n",
      "[13860.]\n",
      "5.221978956475628\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    test_ids = []\n",
    "    predictions = []\n",
    "    for i in range(len(testData[0])):\n",
    "        test_ids.append(str(testData[0][i]))\n",
    "        predictions.append(str(10**p[i]))\n",
    "    \n",
    "    createSubmission(test_ids, predictions)\n",
    "    \n",
    "    print_tree(dT)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
