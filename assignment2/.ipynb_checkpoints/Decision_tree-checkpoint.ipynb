{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Building a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a skeleton of a decision tree classifier for the example data set in `data/example.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "from statistics import median, mode, mean\n",
    "from collections import Counter\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "\n",
    "#A = [[1,2,3],[1,2,3],[1,2,3]]\n",
    "\n",
    "#test = set([A[idx][1] for idx in range(len(A))])\n",
    "#print(test)\n",
    "#print(A[:,0])\n",
    "#test2 = {}\n",
    "#test2[1] = []\n",
    "#temp = np.where( A[:,0] <= 3 )\n",
    "#print(temp[0])\n",
    "\n",
    "#test2[1].append(temp[0])\n",
    "#listi = np.array(test2[1][0].tolist())\n",
    "#print(listi)\n",
    "#print(listi[0])\n",
    "#l = [idx for idx in listi]\n",
    "#print(l)\n",
    "\n",
    "#ls = [1,1,1,2,2,2,3,4,5,5,8,8]\n",
    "#print([ls.count(ls[i]) for i in range(len(ls))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some simple type definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrType(Enum):\n",
    "    cat = 0  # categorical (qualitative) attribute\n",
    "    num = 1  # numerical (quantitative) attribute\n",
    "    target = 2  # target label\n",
    "\n",
    "\n",
    "class NodeType(Enum):\n",
    "    root = 0\n",
    "    internal = 1\n",
    "    leaf = 2\n",
    "\n",
    "\n",
    "class SplitType(Enum):\n",
    "    bin = 0  # binary split\n",
    "    multi = 1  # multi-way split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, some basic classes to represent an attribute, a spltting procedure, and a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attribute(object):\n",
    "    def __init__(self, label, type):\n",
    "        assert type in AttrType\n",
    "        self.label = label\n",
    "        self.type = type\n",
    "        self.stat = None  # holds mean for numerical and mode for categorical attributes\n",
    "\n",
    "\n",
    "class Splitting(object):\n",
    "    def __init__(self, attr, infogain, split_type, cond, splits):\n",
    "        self.attr = attr  # attribute ID (index in ATTR)\n",
    "        self.infogain = infogain  # information gain if splitting is done on this attribute\n",
    "        self.split_type = split_type  # one of SplitType\n",
    "        self.cond = cond  # splitting condition, i.e., values on outgoing edges\n",
    "        self.splits = splits  # list of training records (IDs) for each slitting condition\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, id, type, parent_id, children=None, edge_value=None, val=None, split_type=None, split_cond=None,\n",
    "                 infogain=None):\n",
    "        self.id = id  # ID (same as the index in DT.model list)\n",
    "        self.type = type  # one of NodeType\n",
    "        self.parent_id = parent_id  # ID of parent node (None if root)\n",
    "        self.children = children  # list of IDs of child nodes\n",
    "        self.edge_value = edge_value  # the value of the incoming edge (only if not root node)\n",
    "        self.val = val  # if root or internal node: the attribute that is compared at that node; if leaf node: \n",
    "        #the target value\n",
    "        self.split_type = split_type  # one of SplitType\n",
    "        self.split_cond = split_cond  # splitting condition (median value for binary splits on numerical values; otherwise a list of categorical values (corresponding to child nodes))\n",
    "        self.infogain = infogain\n",
    "\n",
    "    def append_child(self, node_id):\n",
    "        self.children.append(node_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input filename is hard-coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFILE = \"data/housing_price_train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute labels types are hard-coded too (the same order as in the file!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "#Read from training data:\n",
    "with open(INFILE) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    row1 = next(csv_reader)\n",
    "    labels = []\n",
    "    types = []\n",
    "    ATTR = []\n",
    "    \n",
    "    for x in row1:\n",
    "        labels.append(x)\n",
    "    \n",
    "    types = [AttrType.num, AttrType.num, AttrType.cat, AttrType.num, AttrType.num, AttrType.cat, AttrType.cat, AttrType.cat,\n",
    "            AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat,\n",
    "            AttrType.cat, AttrType.num, AttrType.num, AttrType.num, AttrType.num, AttrType.cat, AttrType.cat, AttrType.cat,\n",
    "            AttrType.cat, AttrType.cat, AttrType.num, AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat,\n",
    "            AttrType.cat, AttrType.cat, AttrType.num, AttrType.cat, AttrType.num, AttrType.num, AttrType.num, AttrType.cat,\n",
    "            AttrType.cat, AttrType.cat, AttrType.cat, AttrType.num, AttrType.num, AttrType.num, AttrType.num, AttrType.num,\n",
    "            AttrType.num, AttrType.num, AttrType.num, AttrType.num, AttrType.num, AttrType.cat, AttrType.num, AttrType.cat,\n",
    "            AttrType.num, AttrType.cat, AttrType.cat, AttrType.num, AttrType.cat, AttrType.num, AttrType.num, AttrType.cat,\n",
    "            AttrType.cat, AttrType.cat, AttrType.num, AttrType.num, AttrType.num, AttrType.num, AttrType.num, AttrType.num,\n",
    "            AttrType.cat, AttrType.cat, AttrType.cat, AttrType.num, AttrType.num, AttrType.num, AttrType.cat, AttrType.cat,\n",
    "            AttrType.target]\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        ATTR.append(Attribute(labels[i],types[i]))\n",
    "        \n",
    "    print(len(labels))\n",
    "    print(len(ATTR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index of the target attribute (assuming it's the last)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_TARGET = len(ATTR) - 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A main class DT representing the decision tree classifier. It could represent with methods:\n",
    "\n",
    "  - a given impurity measure;\n",
    "  - the search for the best attribute to split with;\n",
    "  - the addition of a node to the tree;\n",
    "  - a convenient model printer;\n",
    "  - the recursive call for obtaining a tree;\n",
    "  - a builder and an applier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DT(object):\n",
    "    def __init__(self):\n",
    "        self.data = None  # training data set (loaded into memory)\n",
    "        self.model = None  # decision tree model\n",
    "        self.default_target = 0.0  # default target class\n",
    "\n",
    "    def __load_data(self):\n",
    "        with open(INFILE) as csvfile:\n",
    "            self.data = []\n",
    "            csvreader = csv.reader(csvfile, delimiter=',')\n",
    "            next(csvreader)\n",
    "            for row in csvreader:\n",
    "                rec = []\n",
    "                for i in range(len(ATTR)):\n",
    "                    val = row[i].strip()\n",
    "                    #print(val)\n",
    "                    # convert numerical attributes\n",
    "                    if val != 'NA':\n",
    "                        if ATTR[i].type == AttrType.num:  # Note that this will break for \"?\" (missing attribute)\n",
    "                            val = float(val)\n",
    "                    rec.append(val)\n",
    "                self.data.append(rec)\n",
    "                # self.data.append([element.strip() for element in row])  # strip spaces\n",
    "\n",
    "\n",
    "    \n",
    "    def __mean_squared_error(self, records):\n",
    "        \"\"\"\n",
    "        Calculates mean squared error for a selection of records.\n",
    "\n",
    "        :param records: Data records (given by indices)\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        predictions = self.predict(records)\n",
    "        sqe = np.zeros(len(records)) #square error\n",
    "        i = 0\n",
    "        for rec in records:\n",
    "            sqe[i] = (self.data[records,IDX_TARGET]-predictions[records, IDX_TARGET])**2 \n",
    "            i = i+1\n",
    "        MSE = mean(sqe) #mean square error\n",
    "        return MSE\n",
    "    \n",
    "    def __entropy(self, records, attribute):\n",
    "        \"\"\"\n",
    "        Calculates entropy for a selection of records.\n",
    "\n",
    "        :param records: Data records (given by indices)\n",
    "        \"\"\"\n",
    "        uniques = []\n",
    "        ent = 0\n",
    "        for rec in records:\n",
    "            data.append(self.data[rec])\n",
    "        col_vals=[row[80] for row in data\n",
    "        counts = [col_vals.count(col_vals[i]) for i in range(len(col_vals))]\n",
    "        for x in counts:\n",
    "            if x not in uniques:\n",
    "                uniques.append(x)\n",
    "        ent = 0\n",
    "        for x in uniques:\n",
    "            temp = x/len(records)*math.log(x/len(records))\n",
    "            ent = ent - temp\n",
    "        return ent\n",
    "\n",
    "    def __find_best_attr(self, attrs, records):\n",
    "        \"\"\"\n",
    "        Finds the attribute with the largest gain.\n",
    "\n",
    "        :param attrs: Set of attributes\n",
    "        :param records: Training set (list of record ids)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #mse_p = self.__mean_squared_error(records)  # parent's MSE\n",
    "        \n",
    "        splittings = []  # holds the splitting information for each attribute\n",
    "        numKey = 0\n",
    "        for a in attrs:\n",
    "            assert ATTR[a].type in AttrType\n",
    "            splits = {}  # record IDs corresponding to each split\n",
    "            # splitting condition depends on the attribute type\n",
    "            if ATTR[a].type == AttrType.target:  # skip target attribute\n",
    "                continue\n",
    "            elif ATTR[a].type == AttrType.cat:  # categorical attribute\n",
    "                # multi-way split on each possible value\n",
    "                split_mode = SplitType.multi\n",
    "                # each possible attr value corresponds to a split (indexed with categorical labels)\n",
    "                # Note: it's important to consider attr values from the entire training set\n",
    "                split_cond = set([self.data[idx][a] for idx in range(len(self.data))])\n",
    "                \n",
    "                # TODO collect training records for each split \n",
    "                # `splits[val]` holds a list of records for a given split,\n",
    "                # where `val` is an element of `split_cond`\n",
    "                for val in split_cond: #CATEGORICAL SPLIT\n",
    "                    splits[val] = []\n",
    "                    #split into unique values of split_cond:\n",
    "                    for i, j in enumerate(self.data):\n",
    "                        if j[a] == val:\n",
    "                            splits[val].append(i)\n",
    "                    splits[val] = [float(row) for row in splits[val]]\n",
    "                    \n",
    "                    \n",
    "            elif ATTR[a].type == AttrType.num:  # numerical attribute => binary split on median value\n",
    "                print(ATTR[a].type)\n",
    "                #print([row[a] for row in self.data])\n",
    "                split_mode = SplitType.bin\n",
    "                temp_med = [float(row[a]) for row in self.data]\n",
    "                split_cond = median(temp_med)  # (i.e., if less or equal than this value)\n",
    "                # TODO collect training records for each split (in `splits`)\n",
    "                splits[split_cond] = [] #NUMERICAL SPLIT\n",
    "                #index 0 for np.where gir value og datatype:\n",
    "                temp_l = np.where(self.data[:,a]<=split_cond)[0]\n",
    "                temp_u = np.where(self.data[:,a]<=split_cond)[0]\n",
    "                list_l = [float(row) for row in temp_l]\n",
    "                list_u = [float(row) for row in temp_u]\n",
    "                splits[split_cond].append(list_l) # index 0: <= median\n",
    "                splits[split_cond].append(list_u) # index 1: > \n",
    "                numKey = split_cond\n",
    "            # TODO compute gain for attribute a\n",
    "            ent_p = self.__entropy(records,a)    #parent's entropy\n",
    "            infogain = 0\n",
    "            if len(splits)==1: #NUMERIC INFOGAIN\n",
    "                p1 = len(splits[numKey][0])/len(records)\n",
    "                p2 = len(splits[numKey][1])/len(records)\n",
    "                ent_c1 = self.__entropy(splits[numKey][0],a)\n",
    "                ent_c2 = self.__entropy(splits[numkey][1],a)\n",
    "                infogain = ent_p - (p1*ent_c1+p2*ent_c2)\n",
    "            elif len(splits)>1: #CATEGORICAL INFOGAIN\n",
    "                for x in splits:\n",
    "                    for y in x:\n",
    "                        pi = len(y)/len(records)\n",
    "                        ent_ci = self.__entropy(y)\n",
    "                        infogain = infogain + pi*ent_ci\n",
    "                infogain = ent_p - infogain\n",
    "            \n",
    "            splitting = Splitting(a, infogain, split_mode, split_cond, splits)\n",
    "            splittings.append(splitting)\n",
    "\n",
    "        # find best splitting (highest infogain is first element in reversed list)\n",
    "        best_splitting = sorted(splittings, key=lambda x: x.infogain, reverse=True)[0]\n",
    "        return best_splitting\n",
    "\n",
    "    def __add_node(self, parent_id, node_type=NodeType.internal, edge_value=None, val=None, split_type=None,\n",
    "                   split_cond=None):\n",
    "        \"\"\"\n",
    "        Adds a node to the decision tree.\n",
    "\n",
    "        :param parent_id:\n",
    "        :param node_type:\n",
    "        :param edge_value:\n",
    "        :param val:\n",
    "        :param split_type:\n",
    "        :param split_cond:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        node_id = len(self.model)  # id of the newly assigned node\n",
    "        if not self.model:  # the tree is empty\n",
    "            node_type = NodeType.root\n",
    "\n",
    "        node = Node(node_id, node_type, parent_id, children=[], edge_value=edge_value, val=val, split_type=split_type,\n",
    "                    split_cond=split_cond)\n",
    "        self.model.append(node)\n",
    "\n",
    "        # also add it as a child of the parent node\n",
    "        if parent_id is not None:\n",
    "            self.model[parent_id].append_child(node_id)\n",
    "\n",
    "        return node_id\n",
    "\n",
    "    def __id3(self, attrs, records, parent_id=None, value=None):\n",
    "        \"\"\"\n",
    "        Function ID3 that returns a decision tree.\n",
    "\n",
    "        :param attrs: Set of attributes\n",
    "        :param records: Training set (list of record ids)\n",
    "        :param parent_id: ID of parent node\n",
    "        :param value: Value corresponding to the parent attribute, i.e., label of the edge on which we arrived to this node\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # empty training set or empty set of attributes => create leaf node with default class\n",
    "        if not records or not attrs:\n",
    "            self.__add_node(parent_id, node_type=NodeType.leaf, edge_value=value, val=self.default_class)\n",
    "            return\n",
    "\n",
    "        # if all records have the same target value => create leaf node with that target value\n",
    "        same = all(self.data[idx][IDX_TARGET] == self.data[records[0]][IDX_TARGET] for idx in records)\n",
    "        if same:\n",
    "            target = self.data[records[0]][IDX_TARGET]\n",
    "            self.__add_node(parent_id, node_type=NodeType.leaf, edge_value=value, val=target)\n",
    "            return\n",
    "\n",
    "        # find the attribute with the largest gain\n",
    "        splitting = self.__find_best_attr(attrs, records)\n",
    "        # add node\n",
    "        node_id = self.__add_node(parent_id, edge_value=value, val=splitting.attr, split_type=splitting.split_type,\n",
    "                                  split_cond=splitting.cond)\n",
    "        # TODO call tree construction recursively for each split\n",
    "        remAttrs = set(range(len(ATTR) - 1))\n",
    "        remAttrs.remove(splitting.attr)\n",
    "        if len(remAttrs>0):\n",
    "            for split in splitting.splits:\n",
    "                self.__id3(remAttrs, split, node_id, split[0])#TODO siste input skal v√¶re de unike verdiene i splitsa\n",
    "        return\n",
    "        \n",
    "    def print_model(self, node_id=0, level=0):\n",
    "        node = self.model[node_id]\n",
    "        indent = \"  \" * level\n",
    "        if node.type == NodeType.leaf:\n",
    "            print(indent + str(node.edge_value) + \" [Leaf node] class=\" + node.val)\n",
    "        else:\n",
    "            cond = \" <= \" + str(node.split_cond) if ATTR[node.val].type == AttrType.num else \" == ? \"\n",
    "            if node.type == NodeType.root:\n",
    "                print(\"[Root node] '\" + ATTR[node.val].label + \"'\" + cond)\n",
    "            else:\n",
    "                print(indent + str(node.edge_value) + \" [Internal node] '\" + ATTR[node.val].label + \"'\" + cond)\n",
    "            # print tree for child notes recursively\n",
    "            for n_id in node.children:\n",
    "                self.print_model(n_id, level + 1)\n",
    "\n",
    "    def build_model(self):\n",
    "        self.__load_data()\n",
    "        self.model = []  # holds the decision tree model, represented as a list of nodes\n",
    "        # Get majority class\n",
    "        #   Note: Counter returns a dictionary, most_common(x) returns a list with the x most common elements as\n",
    "        #         (key, count) tuples; we need to take the first element of the list and the first element of the tuple \n",
    "        \n",
    "        #l = [float(row) for row in range(self.data) if row[0] == 5]\n",
    "        #print(l)\n",
    "        #probs = []\n",
    "        #data = self.data[:2]\n",
    "        #print(self.data[:2]\n",
    "        #col_vals=[float(col[IDX_TARGET]) for col in data]\n",
    "        #for x in col_vals:\n",
    "        #    if x not in probs:\n",
    "        #        probs.append(x)\n",
    "        #print(probs)\n",
    "        \n",
    "        \n",
    "        target_vals=[float(col[IDX_TARGET]) for col in self.data]\n",
    "        self.default_target = mean(target_vals)\n",
    "        self.__id3(set(range(len(ATTR) - 1)), list(range(len(self.data))))\n",
    "\n",
    "    def apply_model(self, record):\n",
    "        node = self.model[0]\n",
    "        while node.type != NodeType.leaf:\n",
    "            \n",
    "        \t# TODO based on the value of the record's attribute that is tested in `node`,\n",
    "        \t# set `node` to one of its child nodes until a leaf node is reached\n",
    "            \n",
    "            print(\"test\")\n",
    "        return node.val\n",
    "    \n",
    "    def predict(self, records):\n",
    "        predictions = []\n",
    "        for record in records:\n",
    "            pred_val = self.apply_model(record)\n",
    "            \n",
    "            # TODO append pred_val to predictions\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSubmission(test_ids, predictions):\n",
    "    sub = pd.DataFrame()\n",
    "    sub['Id'] = test_ids\n",
    "    sub['SalePrice'] = predictions\n",
    "    sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the main function building a decision tree model, printing it and applying it on some unseen records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model:\n",
      "AttrType.num\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-288-bb9a3ca47ae3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-288-bb9a3ca47ae3>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Build model:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-286-6c826a79a7f4>\u001b[0m in \u001b[0;36mbuild_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0mtarget_vals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mIDX_TARGET\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__id3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mATTR\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-286-6c826a79a7f4>\u001b[0m in \u001b[0;36m__id3\u001b[1;34m(self, attrs, records, parent_id, value)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;31m# find the attribute with the largest gain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m         \u001b[0msplitting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__find_best_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;31m# add node\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         node_id = self.__add_node(parent_id, edge_value=value, val=splitting.attr, split_type=splitting.split_type,\n",
      "\u001b[1;32m<ipython-input-286-6c826a79a7f4>\u001b[0m in \u001b[0;36m__find_best_attr\u001b[1;34m(self, attrs, records)\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[0msplits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit_cond\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#NUMERICAL SPLIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[1;31m#index 0 for np.where gir value og datatype:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                 \u001b[0mtemp_l\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0msplit_cond\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m                 \u001b[0mtemp_u\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0msplit_cond\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[0mlist_l\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_l\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    dt = DT()\n",
    "    #l = [idx for idx in range(len(dt.__load_data())) if idx[0] == 1]\n",
    "    #print(l)\n",
    "    \n",
    "    print(\"Build model:\")\n",
    "    dt.build_model()\n",
    "    dt.print_model()\n",
    "\n",
    "    print(\"\\nApply model:\")\n",
    "    print(dt.apply_model([1461,20,'RH',80,11622,'Pave','NA','Reg','Lvl','AllPub','Inside','Gtl','NAmes','Feedr','Norm','1Fam','1Story',5,6,1961,1961,\n",
    "                          'Gable','CompShg','VinylSd','VinylSd',None,0,'TA','TA','CBlock','TA','TA','No','Rec',468,'LwQ',144,270,882,'GasA','TA','Y',\n",
    "                          'SBrkr',896,0,0,896,0,0,1,0,2,1,'TA',5,'Typ',0,'NA','Attchd',1961,'Unf',1,730,'TA','TA','Y',140,0,0,0,120,0,'NA',\n",
    "                          'MnPrv','NA',0,6,2010,'WD','Normal']))\n",
    "    \n",
    "    print(dt.apply_model(['sunny', 85, 85, 'false']))\n",
    "    print(dt.apply_model(['overcast', 75, 85, 'true']))\n",
    "    print(dt.apply_model(['rain', 75, 85, 'false']))\n",
    "\n",
    "    createSubmission(test_ids, dt.predict(test_data))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
