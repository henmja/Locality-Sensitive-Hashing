{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = np.loadtxt('data/housing_price_train.csv',dtype='<U20',delimiter=',')\n",
    "testdata = np.loadtxt('data/housing_price_test.csv',dtype='<U20',delimiter=',')\n",
    "\n",
    "test_pre_attr_names = testdata[0]\n",
    "test_attr_names = test_pre_attr_names[1:]\n",
    "pre_attr_names = alldata[0]\n",
    "pre_data= np.transpose(np.copy(alldata[1:]))\n",
    "test_data = np.transpose(np.copy(testdata[1:]))\n",
    "test_data = np.copy(test_data)\n",
    "test_attr_names =np.copy(test_pre_attr_names)\n",
    "prices = np.log10(np.float64(pre_data[-1]))\n",
    "pre_data= np.copy(pre_data[1:-1])\n",
    "pre_attr_names = np.copy(pre_attr_names[1:-1])\n",
    "neigh_ind = np.where(pre_attr_names=='Neighborhood')[0][0]\n",
    "neighborhoods = np.unique(pre_data[neigh_ind])\n",
    "bad_attrs= np.array(['3SsnPorch','Alley','MoSold','PoolQC',\n",
    "                     'Utilities','YrSold','LandSlope',\n",
    "                     'LotFrontage','MasVnrType','MasVnrArea','BsmtQual',\n",
    "                     'BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n",
    "                     'Electrical','FireplaceQu','GarageType','GarageYrBlt',\n",
    "                     'GarageFinish','GarageQual','GarageCond','Fence','MiscFeature',\n",
    "                     'PoolArea','Street',])\n",
    "                     #'1stFlrSF','2ndFlrSF','BsmtFinSF1','BsmtFinSF2','Neighborhood'])\n",
    "#last row of things is getting rid of redundancies\n",
    "good_attrs = np.array([[pre_attr_names[i],i]  for i in range(len(pre_attr_names)) if (pre_attr_names[i] not in bad_attrs)])\n",
    "attr_names = good_attrs[:,0]\n",
    "data = pre_data[np.int64(good_attrs[:,1])]\n",
    "data_by_neighborhood ={} \n",
    "price_by_neighborhood = {}\n",
    "for neighborhood in neighborhoods:\n",
    "    neighbors_inds = np.where(data[neigh_ind,:] ==neighborhood)[0]\n",
    "    neighbors_data = data[:,neighbors_inds]\n",
    "    neighbors_prices = prices[neighbors_inds]\n",
    "    data_by_neighborhood[neighborhood] =[neighbors_data,neighbors_data,neighbors_prices]\n",
    "def fixna(attr):\n",
    "    return\n",
    "def plotcor(vals,prices,name):\n",
    "    try:\n",
    "        uvals = np.copy(np.float64(vals))\n",
    "        plt.plot(uvals,prices,'o')\n",
    "        plt.xlabel(name)\n",
    "        plt.ylabel('log(Price)')\n",
    "        plt.savefig('cor/'+name+'_cor.png') \n",
    "        plt.close()\n",
    "    except:\n",
    "        uniq_vals,map_ints,val_counts,map_attr = name_to_number(vals)\n",
    "        plt.plot(map_attr, prices,'o')\n",
    "        print(name)\n",
    "        plt.xlabel(name)\n",
    "        plt.ylabel('log(Price)')\n",
    "        plt.savefig('cor/'+name+'_cor.png') \n",
    "        plt.close()\n",
    "def loopcor():\n",
    "    for i in range(len(attr_names)):\n",
    "        plotcor(data[i],prices,attr_names[i])\n",
    "\n",
    "def name_to_number(attribute):\n",
    "    uniq_vals, val_counts = np.unique(attribute,return_counts=True)\n",
    "    map_ints = range(len(uniq_vals))\n",
    "    map_attr = np.copy(attribute)\n",
    "    for i in range(len(uniq_vals)):\n",
    "        all_val = np.where(attribute == uniq_vals[i])[0]\n",
    "        map_attr[all_val] = map_ints[i]\n",
    "        intvals = map_ints[i]\n",
    "        allints = np.ones_like(all_val)*intvals\n",
    "    return [uniq_vals, map_ints, val_counts,map_attr]\n",
    "def counts_arr(attribute):\n",
    "    ''' \n",
    "    Return the unique values and their probabilities\n",
    "    '''\n",
    "    uniq_vals, val_counts=np.unique(attribute,return_counts=True)\n",
    "    val_counts = np.float64(val_counts)\n",
    "    val_probs = val_counts/np.float64(attribute.size)\n",
    "    return uniq_vals, val_probs\n",
    "def get_max_prob(probs):\n",
    "    mxprob= np.where(probs==np.max(probs))[0][0]\n",
    "    return probs[mxprob]\n",
    "def entropy(attribute):\n",
    "    uniq_vals , val_probs =counts_arr(attribute)\n",
    "    return np.sum(val_probs *np.log2(1./val_probs))\n",
    "def info_gain(attribute,labels):\n",
    "    cts_arr =counts_arr(attribute)\n",
    "    label_entropy = entropy(labels)\n",
    "    label_given_attr = []\n",
    "    if type(cts_arr[1]) != np.ndarray:\n",
    "        return label_entropy\n",
    "    cts_dict = dict(zip(cts_arr[0],cts_arr[1]))\n",
    "    for uniq_attr_val,uniq_val_prob in cts_dict.items():\n",
    "        label_given_attr.append( uniq_val_prob *entropy(labels[attribute==uniq_attr_val]))\n",
    "    return label_entropy- np.sum(label_given_attr)\n",
    "def best_attr(data, attribute_names,labels):\n",
    "    gains=np.array([])\n",
    "    for i in data:\n",
    "        gains=np.append( gains, info_gain(i, labels))\n",
    "    maxind = np.where(gains == np.max(gains))[0][0]\n",
    "    maxgn = gains[maxind]\n",
    "    att_name= attribute_names[maxind]\n",
    "    print('Max Gain', maxgn)\n",
    "    return att_name,maxind\n",
    "class Tree:\n",
    "    def __init__(self, attr_split=None,parent=None):\n",
    "        self.parent=parent\n",
    "        self.child=[]\n",
    "        self.attr_split= attr_split\n",
    "    def add_child(self,child):\n",
    "        self.child.append(child)\n",
    "        child.parent= self\n",
    "def finite_combo(arr1,arr2):\n",
    "    finds = np.where( (~np.isnan(arr1)) &(~np.isnan(arr2)))[0]\n",
    "    fin_arr1 = arr1[finds]\n",
    "    fin_arr2 = arr2[finds]\n",
    "    return fin_arr1,fin_arr2,finds\n",
    "def splitdata(attr,labels,attr_name,showplot=False,depth=0):\n",
    "    ''' \n",
    "    Trying to find best spot to split at\n",
    "    '''\n",
    "    try:\n",
    "        attr_dat =np.float64(attr)\n",
    "        uniq_vals,probs = counts_arr(attr)\n",
    "        typ='num'    \n",
    "    except:\n",
    "        uniq_vals,map_ints, val_counts, map_attr = name_to_number(attr)\n",
    "        attr_dat = np.copy(np.float64(map_attr))\n",
    "        typ='nom'\n",
    "        print('Map Attr',map_attr)\n",
    "    if typ =='nom' or len(uniq_vals) <=10:\n",
    "        n_splits = len(uniq_vals)\n",
    "        splits =[]\n",
    "        split_ints = []\n",
    "        for i in range(n_splits):\n",
    "            splits.append([attr==uniq_vals[i] ])\n",
    "            split_ints.append(uniq_vals[i])\n",
    "        threshold_val = np.array([-99])\n",
    "        threshold_type ='NA'\n",
    "    elif len(uniq_vals) >2:\n",
    "        try:\n",
    "            labels = np.float64(labels)\n",
    "            sort_by_attr = np.argsort(attr_dat)\n",
    "            attr_sort = np.copy(attr_dat[sort_by_attr])\n",
    "            label_sort =np.copy(labels[sort_by_attr])\n",
    "            linregslopes_left = []\n",
    "            linregslopes_right = []\n",
    "            for i in range(len(attr_sort)//10,len(attr_sort) -len(attr_sort)//10  ):\n",
    "                attr_l = attr_sort[0:i]\n",
    "                attr_r = attr_sort[i:]\n",
    "                lab_l = label_sort[0:i]\n",
    "                lab_r = label_sort[i:]\n",
    "                m_l,b_l = np.polyfit(attr_l,lab_l,1)\n",
    "                m_r,b_r = np.polyfit(attr_r,lab_r,1)\n",
    "                linregslopes_left.append(abs(m_l))\n",
    "                #ind_left.append(i)\n",
    "                linregslopes_right.append(abs(m_r))\n",
    "                #ind_right.append(i)\n",
    "            linregslopes_left =np.array(linregslopes_left)\n",
    "            linregslopes_right =np.array(linregslopes_right)\n",
    "            finite_left,finite_right,finite_inds = finite_combo(linregslopes_left,linregslopes_right)\n",
    "            inv_left = 1./finite_left\n",
    "            inv_right = 1./finite_right\n",
    "            #lin left = where left side is linear and right is not\n",
    "            linleft=finite_left*inv_right\n",
    "            #linright = where right side is linear and left is not\n",
    "            linright = inv_left*finite_right\n",
    "            finmxleft = np.where(linleft == np.max(linleft))[0][0]\n",
    "            mxleft = np.where(linregslopes_left == finite_left[finmxleft])[0]\n",
    "            #transforming back to the array with infinite (if necessary)\n",
    "            finmxright = np.where(linright == np.max(linright))[0][0]\n",
    "            mxright = np.where(linregslopes_right == finite_right[finmxright])[0]\n",
    "            #if mxleft >mxright:\n",
    "            threshold_val = attr_sort[mxleft+len(attr_sort)//10]\n",
    "            #else:\n",
    "            #    threshold_val = attr_sort[mxright+len(attr_sort)//10]        \n",
    "            if typ=='nom':\n",
    "                split_l = [attr_dat <threshold_val]\n",
    "                split_r = [attr_dat >=threshold_val]\n",
    "                splits=[split_l,split_r]\n",
    "            else:\n",
    "                split_l = [attr_dat<=threshold_val]\n",
    "                split_r= [attr_dat >threshold_val]\n",
    "                splits=[split_l,split_r]\n",
    "            threshold_type='linsplit'\n",
    "            split_ints =[]\n",
    "        except:\n",
    "            threshold_val = np.array([np.mean(attr_dat)])\n",
    "            split_l =[attr_dat < threshold_val]\n",
    "            split_r = [attr_dat >= threshold_val]\n",
    "            splits = [split_l,split_r]\n",
    "            threshold_type = 'linsplit'\n",
    "            split_ints = []\n",
    "    if showplot: \n",
    "        plt.plot(attr_dat, labels,'o')\n",
    "        plt.axvline(x=threshold_val,label='Threshold = '+str(threshold_val))\n",
    "        plt.ylabel('log(Price))')\n",
    "        plt.xlabel(attr_name)\n",
    "        plt.legend()\n",
    "        plt.savefig('thresholds/'+attr_name +'_threshold_depth'+str(depth)+'png')\n",
    "        plt.close()\n",
    "    return splits,threshold_val,threshold_type,split_ints,typ #threshold_val, mxleft, mxright\n",
    "def pure(vals):\n",
    "    return len(np.unique(vals))==1\n",
    "def decisionTree(dat,attribute_names,labels,root=None,depth=0):\n",
    "    '''\n",
    "    get bestattr, threshold for split, and then values\n",
    "    which align along the splits and repeat\n",
    "    '''\n",
    "    if dat[0] ==[]:\n",
    "        root.avg_label = root.parent.avg_label\n",
    "        return #root#root.parent.avg_label\n",
    "    if len(dat[0]) <5:\n",
    "        root.avg_label= np.mean(labels)#root#return root.parent.avg_label\n",
    "        return\n",
    "    bestattr,maxind= best_attr(dat,attribute_names,labels)\n",
    "    if not root:\n",
    "        root = Tree()\n",
    "    root.split_attr =bestattr\n",
    "    root.split_attr_ind=maxind\n",
    "    attr_data = np.copy(dat[maxind])\n",
    "    if pure(attr_data):\n",
    "        root.avg_label= np.mean(labels)\n",
    "        return \n",
    "    splits,threshold_val, threshold_type,split_ints,typ=splitdata(attr_data,labels,bestattr,showplot=False,depth=depth)\n",
    "    trees = []\n",
    "    for i in range(len(splits)):\n",
    "        trees.append(Tree(parent=root))\n",
    "        root.add_child(trees[i])\n",
    "        root.split_ints = split_ints\n",
    "        trees[i].inds_filt = splits[i][0]\n",
    "        \n",
    "    root.threshold_val =threshold_val\n",
    "    root.threshold_type = threshold_type\n",
    "    root.typ = typ\n",
    "    root.avg_label= np.mean(labels)\n",
    "    depth+=1\n",
    "    red= [attribute_names!=bestattr]\n",
    "\n",
    "    red_attr_names = attribute_names[red]\n",
    "    red_data = dat[red]\n",
    "    for tree in trees:\n",
    "        decisionTree(red_data[:,tree.inds_filt],red_attr_names,labels[tree.inds_filt],root=tree,depth=depth)\n",
    "    return root \n",
    "price_by_id = {}\n",
    "dT = decisionTree(data,attr_names, prices)\n",
    "def predictlabels(dat,attr_names,dTree,predTree = None,labels=[]):\n",
    "    if len(dat[0]) == 0:\n",
    "        return\n",
    "    if len(labels) == 0:\n",
    "        labels = np.ones_like(dat[0])\n",
    "    if len(dTree.child) ==0:\n",
    "        for tid in dat[0]:\n",
    "            price_by_id[tid] = dTree.avg_label \n",
    "        return\n",
    "    if not predTree:\n",
    "        predTree = Tree()\n",
    "    attr_split = dTree.split_attr \n",
    "    attr_split_ind =np.where(attr_names == attr_split)[0]\n",
    "    attr_split_ints = dTree.split_ints\n",
    "    attr_dat = dat[attr_split_ind][0] #these are still stored as strings\n",
    "    thresh_type = dTree.threshold_type\n",
    "    thresh_val = np.float64(dTree.threshold_val[0])\n",
    "    try:\n",
    "        attr_data =np.float64(attr_dat)\n",
    "        uniq_vals,probs = counts_arr(attr_dat)\n",
    "    except:\n",
    "        uniq_vals,map_ints, val_counts, map_attr = name_to_number(attr_dat)\n",
    "        attr_data = np.copy(np.float64(map_attr))\n",
    "    typ = dTree.typ\n",
    "    predTree.attr_split = attr_split\n",
    "    if thresh_type=='0':\n",
    "        split_l = np.where(attr_data == 0)[0]\n",
    "        split_r = np.where(attr_data > 0)[0]\n",
    "        splits = [split_l,split_r]\n",
    "    elif thresh_type=='linsplit':\n",
    "        if typ=='nom':\n",
    "            split_l = np.where(attr_data <thresh_val)[0]\n",
    "            split_r = np.where(attr_data >=thresh_val)[0]\n",
    "            splits=[split_l,split_r]\n",
    "        else:\n",
    "            split_l = np.where(attr_data<=thresh_val)[0]\n",
    "            split_r= np.where(attr_data >thresh_val)[0]       \n",
    "            splits=[split_l,split_r]   \n",
    "    elif thresh_type == 'NA':\n",
    "        splits =[]\n",
    "        for val in attr_split_ints:\n",
    "            splits.append(np.where(attr_data==val)[0])    \n",
    "    trees = []\n",
    "    for i in range(len(splits)):\n",
    "        trees.append(Tree(parent=predTree))\n",
    "        predTree.add_child(trees[i])\n",
    "        trees[i].label_inds = splits[i]\n",
    "        trees[i].dtree = dTree.child[i]\n",
    "        trees[i].avg_label =dTree.avg_label\n",
    "        predictlabels(dat[:,trees[i].label_inds],attr_names,trees[i].dtree, predTree= trees[i],labels=labels)\n",
    "    tids = dat[0]\n",
    "    labs =[]\n",
    "    for tid in tids:\n",
    "        try:\n",
    "            labs.append(price_by_id[tid])\n",
    "        except:\n",
    "            labs.append(np.median(list(price_by_id.values() )))\n",
    "    return labs\n",
    "p = predictlabels(test_data, test_attr_names,dT)\n",
    "\n",
    "def writepricebyid():\n",
    "    f = open('data/submission.csv','w+')\n",
    "    f.write('Id,SalePrice\\n')\n",
    "    for i in range(len(test_data[0])):\n",
    "        f.write(str(test_data[0][i])+','+str(10**p[i]) +'\\n')\n",
    "    f.close()\n",
    "writepricebyid()\n",
    "def tree_info(tree):\n",
    "    print(tree.split_attr)\n",
    "    print(tree.threshold_val)\n",
    "    print(tree.avg_label)\n",
    "#END DT2    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    import csv\n",
    "import math\n",
    "from statistics import median, mode, mean\n",
    "from collections import Counter\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "\n",
    "#A = [[1,2,3],[1,2,3],[1,2,3]]\n",
    "\n",
    "#test = set([A[idx][1] for idx in range(len(A))])\n",
    "#print(test)\n",
    "#print(A[:,0])\n",
    "#test2 = {}\n",
    "#test2[1] = []\n",
    "#temp = np.where( A[:,0] <= 3 )\n",
    "#print(temp[0])\n",
    "#def test(x):\n",
    "#    print(x)\n",
    "#    for i in range(0,2):\n",
    "#        test(i)\n",
    "        \n",
    "#test('test')\n",
    "    \n",
    "#test2[1].append(temp[0])\n",
    "#listi = np.array(test2[1][0].tolist())\n",
    "#print(listi)\n",
    "#print(listi[0])\n",
    "#l = [idx for idx in listi]\n",
    "#print(l)\n",
    "\n",
    "#ls = [1,1,1,2,2,2,3,4,5,5,8,8]\n",
    "#print([ls.count(ls[i]) for i in range(len(ls))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AttrType(Enum):\n",
    "    cat = 0  # categorical (qualitative) attribute\n",
    "    num = 1  # numerical (quantitative) attribute\n",
    "    target = 2  # target label\n",
    "\n",
    "\n",
    "class NodeType(Enum):\n",
    "    root = 0\n",
    "    internal = 1\n",
    "    leaf = 2\n",
    "\n",
    "\n",
    "class SplitType(Enum):\n",
    "    bin = 0  # binary split\n",
    "    multi = 1  # multi-way split\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Attribute(object):\n",
    "def __init__(self, label, type):\n",
    "        assert type in AttrType\n",
    "        self.label = label\n",
    "        self.type = type\n",
    "        self.stat = None  # holds mean for numerical and mode for categorical attributes\n",
    "\n",
    "\n",
    "class Splitting(object):\n",
    "    def __init__(self, attr, infogain, split_type, cond, splits):\n",
    "        self.attr = attr  # attribute ID (index in ATTR)\n",
    "        self.infogain = infogain  # information gain if splitting is done on this attribute\n",
    "        self.split_type = split_type  # one of SplitType\n",
    "        self.cond = cond  # splitting condition, i.e., values on outgoing edges\n",
    "        self.splits = splits  # list of training records (IDs) for each slitting condition\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, id, type, parent_id, children=None, edge_value=None, val=None, split_type=None, split_cond=None,\n",
    "                 infogain=None):\n",
    "        self.id = id  # ID (same as the index in DT.model list)\n",
    "        self.type = type  # one of NodeType\n",
    "        self.parent_id = parent_id  # ID of parent node (None if root)\n",
    "        self.children = children  # list of IDs of child nodes\n",
    "        self.edge_value = edge_value  # the value of the incoming edge (only if not root node)\n",
    "        self.val = val  # if root or internal node: the attribute that is compared at that node; if leaf node: \n",
    "        #the target value\n",
    "        self.split_type = split_type  # one of SplitType\n",
    "        self.split_cond = split_cond  # splitting condition (median value for binary splits on numerical values; otherwise a list of categorical values (corresponding to child nodes))\n",
    "        self.infogain = infogain\n",
    "\n",
    "    def append_child(self, node_id):\n",
    "        self.children.append(node_id)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "INFILE = \"data/housing_price_train.csv\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Read from training data:\n",
    "with open(INFILE) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    row1 = next(csv_reader)\n",
    "    labels = []\n",
    "    types = []\n",
    "    ATTR = []\n",
    "    i = 0\n",
    "    for x in row1:\n",
    "        if i>0: #skip first element (ID)\n",
    "            labels.append(x)\n",
    "        i=i+1\n",
    "    \n",
    "    types = [AttrType.num, AttrType.cat, AttrType.num, AttrType.num, AttrType.cat, AttrType.cat, AttrType.cat,\n",
    "            AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat,\n",
    "            AttrType.cat, AttrType.num, AttrType.num, AttrType.num, AttrType.num, AttrType.cat, AttrType.cat, AttrType.cat,\n",
    "            AttrType.cat, AttrType.cat, AttrType.num, AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat,\n",
    "            AttrType.cat, AttrType.cat, AttrType.num, AttrType.cat, AttrType.num, AttrType.num, AttrType.num, AttrType.cat,\n",
    "            AttrType.cat, AttrType.cat, AttrType.cat, AttrType.num, AttrType.num, AttrType.num, AttrType.num, AttrType.num,\n",
    "            AttrType.num, AttrType.num, AttrType.num, AttrType.num, AttrType.num, AttrType.cat, AttrType.num, AttrType.cat,\n",
    "            AttrType.num, AttrType.cat, AttrType.cat, AttrType.num, AttrType.cat, AttrType.num, AttrType.num, AttrType.cat,\n",
    "            AttrType.cat, AttrType.cat, AttrType.num, AttrType.num, AttrType.num, AttrType.num, AttrType.num, AttrType.num,\n",
    "            AttrType.cat, AttrType.cat, AttrType.cat, AttrType.num, AttrType.num, AttrType.num, AttrType.cat, AttrType.cat,\n",
    "            AttrType.target]\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        ATTR.append(Attribute(labels[i],types[i]))\n",
    "        \n",
    "    print(len(labels))\n",
    "    print(len(ATTR))\n",
    "    \n",
    "    \n",
    "IDX_TARGET = len(ATTR) - 1 \n",
    "remAttrs = set(range(len(ATTR) - 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DT(object):\n",
    "    def __init__(self):\n",
    "        self.data = None  # training data set (loaded into memory)\n",
    "        self.model = None  # decision tree model\n",
    "        self.default_target = 0.0  # default target class\n",
    "\n",
    "    def __load_data(self):\n",
    "        with open(INFILE) as csvfile:\n",
    "            self.data = []\n",
    "            csvreader = csv.reader(csvfile, delimiter=',')\n",
    "            next(csvreader) #REMOVE FIRST ROW\n",
    "            for row in csvreader:\n",
    "                iterRow = iter(row)\n",
    "                next(iterRow) #remove first column (IDS)\n",
    "                row = list(iterRow)\n",
    "                rec = []\n",
    "                for i in range(len(ATTR)):\n",
    "                    val = row[i].strip()\n",
    "                    #print(val)\n",
    "                    # convert numerical attributes\n",
    "                    if val == 'NA':\n",
    "                        val = -1\n",
    "                    if ATTR[i].type == AttrType.num:  # Note that this will break for \"?\" (missing attribute)\n",
    "                            val = float(val)\n",
    "                    rec.append(val)\n",
    "                self.data.append(rec)\n",
    "                #self.data.pop(0) #REMOVE FIRST COLUMN (IDS)\n",
    "                #for row in self.data:\n",
    "                #    del row[0] #REMOVE FIRST ROW (LABELS)\n",
    "                #print(self.data[0])\n",
    "                # self.data.append([element.strip() for element in row])  # strip spaces\n",
    "\n",
    "\n",
    "    \n",
    "    def __mean_squared_error(self, records):\n",
    "        \"\"\"\n",
    "        Calculates mean squared error for a selection of records.\n",
    "\n",
    "        :param records: Data records (given by indices)\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        predictions = self.predict(records)\n",
    "        sqe = np.zeros(len(records)) #square error\n",
    "        i = 0\n",
    "        for rec in records:\n",
    "            sqe[i] = (self.data[records,IDX_TARGET]-predictions[records, IDX_TARGET])**2 \n",
    "            i = i+1\n",
    "        MSE = mean(sqe) #mean square error\n",
    "        return MSE\n",
    "    \n",
    "    def __entropy(self, records, attribute):\n",
    "        \"\"\"\n",
    "        Calculates entropy for a selection of records.\n",
    "\n",
    "        :param records: Data records (given by indices)\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        uniques = []\n",
    "        ent = 0\n",
    "        for rec in records:\n",
    "            data.append(self.data[rec])\n",
    "        col_vals=[row[IDX_TARGET] for row in data]\n",
    "        counts = [col_vals.count(col_vals[i]) for i in range(len(col_vals))]\n",
    "        for x in counts:\n",
    "            if x not in uniques:\n",
    "                uniques.append(x)\n",
    "        ent = 0\n",
    "        for x in uniques:\n",
    "            temp = x/len(records)*math.log(x/len(records))\n",
    "            ent = ent - temp\n",
    "        return ent\n",
    "\n",
    "    def __find_best_attr(self, attrs, records):\n",
    "        \"\"\"\n",
    "        Finds the attribute with the largest gain.\n",
    "\n",
    "        :param attrs: Set of attributes\n",
    "        :param records: Training set (list of record ids)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #mse_p = self.__mean_squared_error(records)  # parent's MSE\n",
    "        \n",
    "        splittings = []  # holds the splitting information for each attribute\n",
    "        numKey = 0\n",
    "        for a in attrs:\n",
    "            catKey = []\n",
    "            assert ATTR[a].type in AttrType\n",
    "            splits = {}  # record IDs corresponding to each split\n",
    "            # splitting condition depends on the attribute type\n",
    "            if ATTR[a].type == AttrType.target:  # skip target attribute\n",
    "                continue\n",
    "            elif ATTR[a].type == AttrType.cat:  # categorical attribute\n",
    "                # multi-way split on each possible value\n",
    "                split_mode = SplitType.multi\n",
    "                # each possible attr value corresponds to a split (indexed with categorical labels)\n",
    "                # Note: it's important to consider attr values from the entire training set\n",
    "                split_cond = set([self.data[idx][a] for idx in range(len(self.data))])\n",
    "                #print(len(split_cond))\n",
    "                #print(split_cond)\n",
    "                # TODO collect training records for each split \n",
    "                # `splits[val]` holds a list of records for a given split,\n",
    "                # where `val` is an element of `split_cond`\n",
    "                for val in split_cond:\n",
    "                    splits[val] = [] #CATEGORICAL SPLIT\n",
    "                #index 0 for np.where gir value og datatype:\n",
    "                    data = [row[a] for row in self.data]\n",
    "                    temp_i = np.where(np.asarray(data)==val)[0] #index 0 because index 1 in np.where returns datatype.\n",
    "                    list_i = list(temp_i)\n",
    "                    splits[val] = list_i\n",
    "                    #print(val)\n",
    "                    catKey.append(val)\n",
    "                    \n",
    "            elif ATTR[a].type == AttrType.num:  # numerical attribute => binary split on median value\n",
    "                #print(ATTR[a].type)\n",
    "                #print([row[a] for row in self.data])\n",
    "                split_mode = SplitType.bin\n",
    "                temp_med = [float(row[a]) for row in self.data]\n",
    "                split_cond = median(temp_med)  # (i.e., if less or equal than this value)\n",
    "                # TODO collect training records for each split (in `splits`)\n",
    "                splits[split_cond] = [] #NUMERICAL SPLIT\n",
    "                #index 0 for np.where gir value og datatype:\n",
    "                data = [float(row[a]) for row in self.data]\n",
    "                temp_l = np.where(np.asarray(data)<=split_cond)[0]\n",
    "                temp_u = np.where(np.asarray(data)<=split_cond)[0]\n",
    "                list_l = list(temp_l)\n",
    "                list_u = list(temp_u)\n",
    "                splits[split_cond].append(list_l) # index 0: <= median\n",
    "                splits[split_cond].append(list_u) # index 1: > \n",
    "                numKey = split_cond\n",
    "            # TODO compute gain for attribute a\n",
    "            ent_p = self.__entropy(records,a)    #parent's entropy\n",
    "            infogain = 0\n",
    "            if len(splits)==1: #NUMERIC INFOGAIN\n",
    "                p1 = len(splits[numKey][0])/len(records)\n",
    "                p2 = len(splits[numKey][1])/len(records)\n",
    "                #print(splits[numKey][0])\n",
    "                ent_c1 = self.__entropy(splits[numKey][0],a)\n",
    "                ent_c2 = self.__entropy(splits[numKey][1],a)\n",
    "                infogain = ent_p - (p1*ent_c1+p2*ent_c2)\n",
    "            elif len(splits)>1: #CATEGORICAL INFOGAIN\n",
    "                for key in catKey:\n",
    "                    #print(splits[key])\n",
    "                    pi = len(splits[key])/len(records)\n",
    "                    ent_ci = self.__entropy(splits[key],a)\n",
    "                    infogain = infogain + pi*ent_ci\n",
    "                infogain = ent_p - infogain\n",
    "            #print(splits)\n",
    "            splitting = Splitting(a, infogain, split_mode, split_cond, splits)\n",
    "            splittings.append(splitting)\n",
    "\n",
    "        # find best splitting (highest infogain is first element in reversed list)\n",
    "        best_splitting = sorted(splittings, key=lambda x: x.infogain, reverse=True)[0]\n",
    "        return best_splitting\n",
    "\n",
    "    def __add_node(self, parent_id, node_type=NodeType.internal, edge_value=None, val=None, split_type=None,\n",
    "                   split_cond=None):\n",
    "        \"\"\"\n",
    "        Adds a node to the decision tree.\n",
    "\n",
    "        :param parent_id:\n",
    "        :param node_type:\n",
    "        :param edge_value:\n",
    "        :param val:\n",
    "        :param split_type:\n",
    "        :param split_cond:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        node_id = len(self.model)  # id of the newly assigned node\n",
    "        if not self.model:  # the tree is empty\n",
    "            node_type = NodeType.root\n",
    "\n",
    "        node = Node(node_id, node_type, parent_id, children=[], edge_value=edge_value, val=val, split_type=split_type,\n",
    "                    split_cond=split_cond)\n",
    "        self.model.append(node)\n",
    "\n",
    "        # also add it as a child of the parent node\n",
    "        if parent_id is not None:\n",
    "            self.model[parent_id].append_child(node_id)\n",
    "\n",
    "        return node_id\n",
    "\n",
    "    def __id3(self, attrs, records, parent_id=None, value=None):\n",
    "        \"\"\"\n",
    "        Function ID3 that returns a decision tree.\n",
    "\n",
    "        :param attrs: Set of attributes\n",
    "        :param records: Training set (list of record ids)\n",
    "        :param parent_id: ID of parent node\n",
    "        :param value: Value corresponding to the parent attribute, i.e., label of the edge on which we arrived to this node\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # empty training set or empty set of attributes => create leaf node with default class\n",
    "        if not records or not attrs:\n",
    "            self.__add_node(parent_id, node_type=NodeType.leaf, edge_value=value, val=self.default_target)\n",
    "            return\n",
    "\n",
    "        # if all records have the same target value => create leaf node with that target value\n",
    "        same = all(self.data[idx][IDX_TARGET] == self.data[records[0]][IDX_TARGET] for idx in records)\n",
    "        if same:\n",
    "            target = self.data[records[0]][IDX_TARGET]\n",
    "            self.__add_node(parent_id, node_type=NodeType.leaf, edge_value=value, val=target)\n",
    "            return\n",
    "\n",
    "        # find the attribute with the largest gain\n",
    "        splitting = self.__find_best_attr(attrs, records)\n",
    "        # add node\n",
    "        node_id = self.__add_node(parent_id, edge_value=value, val=splitting.attr, split_type=splitting.split_type,\n",
    "                                  split_cond=splitting.cond)\n",
    "        \n",
    "        # TODO call tree construction recursively for each split\n",
    "        if splitting.attr in remAttrs:\n",
    "            remAttrs.remove(splitting.attr)\n",
    "        if len(remAttrs)>0:\n",
    "            #print(splitting.splits)\n",
    "            print(splitting.splits)\n",
    "            for key in splitting.splits: #FOR KEY IN SPLITTING.SPLITS??\n",
    "                \n",
    "                if key != -1 and self.model[node_id] != NodeType.leaf: #noen categorical verdier er -1 ('NA').\n",
    "                    #CATEGORICAL:\n",
    "                    if len(splitting.splits)>1:\n",
    "                        #print(splitting.splits['Fa'])\n",
    "                        #print(splitting.splits)\n",
    "                        #print(key)\n",
    "                        #print(splitting.splits)\n",
    "                        self.__id3(remAttrs, [rec for rec in splitting.splits[key]], node_id, key)\n",
    "                    #NUMERICAL:\n",
    "                    elif len(splitting.splits)==1:\n",
    "                        for i in range(2):\n",
    "                            val = splitting.cond\n",
    "                            #print(val)\n",
    "                            self.__id3(remAttrs, [rec for rec in splitting.splits[key][i]], node_id, val)\n",
    "        return\n",
    "        \n",
    "    def print_model(self, node_id=0, level=0):\n",
    "        node = self.model[node_id]\n",
    "        indent = \"  \" * level\n",
    "        if node.type == NodeType.leaf:\n",
    "            print(indent + str(node.edge_value) + \" [Leaf node] class=\" + str(node.val))\n",
    "        else:\n",
    "            cond = \" <= \" + str(node.split_cond) if ATTR[node.val].type == AttrType.num else \" == ? \"\n",
    "            if node.type == NodeType.root:\n",
    "                print(\"[Root node] '\" + ATTR[node.val].label + \"'\" + cond)\n",
    "            else:\n",
    "                print(indent + str(node.edge_value) + \" [Internal node] '\" + ATTR[node.val].label + \"'\" + cond)\n",
    "            # print tree for child notes recursively\n",
    "            for n_id in node.children:\n",
    "                self.print_model(n_id, level + 1)\n",
    "\n",
    "    def build_model(self):\n",
    "        self.__load_data()\n",
    "        self.model = []  # holds the decision tree model, represented as a list of nodes\n",
    "        # Get majority class\n",
    "        #   Note: Counter returns a dictionary, most_common(x) returns a list with the x most common elements as\n",
    "        #         (key, count) tuples; we need to take the first element of the list and the first element of the tuple \n",
    "        \n",
    "        #l = [float(row) for row in range(self.data) if row[0] == 5]\n",
    "        #print(l)\n",
    "        #probs = []\n",
    "        #data = self.data[:2]\n",
    "        #print(self.data[:2]\n",
    "        #col_vals=[float(col[IDX_TARGET]) for col in data]\n",
    "        #for x in col_vals:\n",
    "        #    if x not in probs:\n",
    "        #        probs.append(x)\n",
    "        #print(probs)\n",
    "        \n",
    "        \n",
    "        target_vals=[float(col[IDX_TARGET]) for col in self.data]\n",
    "        self.default_target = mean(target_vals)\n",
    "        self.__id3(set(range(len(ATTR)-1)), list(range(len(self.data))))\n",
    "\n",
    "    def apply_model(self, record):\n",
    "        node = self.model[0]\n",
    "        #while node.type != NodeType.leaf:\n",
    "            \n",
    "        \t# TODO based on the value of the record's attribute that is tested in `node`,\n",
    "        \t# set `node` to one of its child nodes until a leaf node is reached\n",
    "            \n",
    "            #print(\"test\")\n",
    "        return node.val\n",
    "    \n",
    "    def predict(self, records):\n",
    "        predictions = []\n",
    "        for record in records:\n",
    "            pred_val = self.apply_model(record)\n",
    "            \n",
    "            # TODO append pred_val to predictions\n",
    "            #kan bruke mean på leaf node som predicted value\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def createSubmission(test_ids, predictions):\n",
    "    sub = pd.DataFrame()\n",
    "    sub['Id'] = test_ids\n",
    "    sub['SalePrice'] = predictions\n",
    "    sub.to_csv('submission.csv',index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def main():\n",
    "    \n",
    "    dt = DT()\n",
    "    #l = [idx for idx in range(len(dt.__load_data())) if idx[0] == 1]\n",
    "    #print(l)\n",
    "    \n",
    "    print(\"Build model:\")\n",
    "    dt.build_model()\n",
    "    dt.print_model()\n",
    "\n",
    "    print(\"\\nApply model:\")\n",
    "    #print(dt.apply_model([1461,20,'RH',80,11622,'Pave','NA','Reg','Lvl','AllPub','Inside','Gtl','NAmes','Feedr','Norm','1Fam','1Story',5,6,1961,1961,\n",
    "                          #'Gable','CompShg','VinylSd','VinylSd',None,0,'TA','TA','CBlock','TA','TA','No','Rec',468,'LwQ',144,270,882,'GasA','TA','Y',\n",
    "                          #'SBrkr',896,0,0,896,0,0,1,0,2,1,'TA',5,'Typ',0,'NA','Attchd',1961,'Unf',1,730,'TA','TA','Y',140,0,0,0,120,0,'NA',\n",
    "                          #'MnPrv','NA',0,6,2010,'WD','Normal']))\n",
    "    \n",
    "    #print(dt.apply_model(['sunny', 85, 85, 'false']))\n",
    "    #print(dt.apply_model(['overcast', 75, 85, 'true']))\n",
    "    #print(dt.apply_model(['rain', 75, 85, 'false']))\n",
    "    \n",
    "    #evaluate med MSE\n",
    "\n",
    "    #createSubmission(test_ids, dt.predict(test_data))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
