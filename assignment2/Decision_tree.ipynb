{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Building a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a skeleton of a decision tree classifier for the example data set in `data/example.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import  math\n",
    "from statistics import median, mode, mean\n",
    "from collections import Counter\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import scipy.stats as st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input filename is hard-coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trainingSet = np.loadtxt('data/housing_price_train.csv',dtype='<U20',delimiter=',')\n",
    "#testSet = np.loadtxt('data/housing_price_test.csv',dtype='<U20',delimiter=',')\n",
    "trainingSet = np.loadtxt('data/train.csv',dtype='<U20',delimiter=',')\n",
    "testSet = np.loadtxt('data/test.csv',dtype='<U20',delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute labels types are hard-coded too (the same order as in the file!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testAttributes = trainingSet[0,:80]\n",
    "#split training set to training set 75% and test set 25%:\n",
    "\n",
    "#testSet = trainingSet[-366:,:]\n",
    "#testTarget = testSet[:,-1] #real target values\n",
    "#testSet = trainingSet[-366:,:80]\n",
    "#testSet = np.vstack((testAttributes,testSet)) #add attribute labels to testSet\n",
    "#trainingSet = trainingSet[:1095,:]\n",
    "\n",
    "\n",
    "testLabels = testSet[0]\n",
    "testAttributes = testLabels[1:]\n",
    "trainingLabels = trainingSet[0]\n",
    "trainingData= np.transpose(np.copy(trainingSet[1:]))\n",
    "testData = np.transpose(np.copy(testSet[1:]))\n",
    "testData = np.copy(testData)\n",
    "testAttributes =np.copy(testLabels)\n",
    "housingPrices = np.log10(np.float64(trainingData[-1]))\n",
    "trainingData = np.copy(trainingData[1:-1])\n",
    "trainingLabels = np.copy(trainingLabels[1:-1])\n",
    "dupAttrs= np.array(['3SsnPorch','Alley','MoSold','PoolQC','Utilities','YrSold','LandSlope','LotFrontage','MasVnrType',\n",
    "                    'MasVnrArea','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Electrical',\n",
    "                    'FireplaceQu','GarageType','GarageYrBlt','GarageFinish','GarageQual','GarageCond','Fence','MiscFeature',\n",
    "                    'PoolArea','Street',])\n",
    "uniqueAttrs = np.array([[trainingLabels[i],i]  for i in range(len(trainingLabels)) if (trainingLabels[i] not in dupAttrs)])\n",
    "uniqueLabels = uniqueAttrs[:,0]\n",
    "trainData = trainingData[np.int64(uniqueAttrs[:,1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index of the target attribute (assuming it's the last)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDX_TARGET = np.log10(np.float64(trainingSet[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A main class DT representing the decision tree classifier. It could represent with methods:\n",
    "\n",
    "  - a given impurity measure;\n",
    "  - the search for the best attribute to split with;\n",
    "  - the addition of a node to the tree;\n",
    "  - a convenient model printer;\n",
    "  - the recursive call for obtaining a tree;\n",
    "  - a builder and an applier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henri\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:143: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Gain: 8.045796138342293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henri\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:173: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "C:\\Users\\henri\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:174: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Gain: 7.446101489756081\n",
      "Max Gain: 6.524561605715065\n",
      "Max Gain: 6.497662618424865\n",
      "Max Gain: 5.65645677569764\n",
      "Max Gain: 5.631615665225583\n",
      "Max Gain: 5.8574319218048085\n",
      "Max Gain: 6.595891163108825\n",
      "Max Gain: 7.149981774453494\n",
      "Max Gain: 7.43752996644357\n",
      "Max Gain: 7.331770310806017\n",
      "Max Gain: 7.21647843135994\n",
      "Max Gain: 7.813184524595746\n",
      "Max Gain: 7.126460214904276\n",
      "Max Gain: 6.247495789386386\n",
      "Max Gain: 5.94770277922009\n",
      "Max Gain: 6.058984089445426\n",
      "Max Gain: 6.87496155606346\n",
      "Max Gain: 7.164965687925738\n",
      "Max Gain: 6.457972024550777\n",
      "Max Gain: 6.11249200111032\n",
      "Max Gain: 5.304682449772211\n",
      "Max Gain: 4.566108939837479\n",
      "Max Gain: 4.436605434317882\n",
      "Max Gain: 3.321928094887362\n",
      "Max Gain: 3.7004397181410926\n",
      "Max Gain: 6.364489555653822\n",
      "Max Gain: 5.720049960644811\n",
      "Max Gain: 5.155399311574898\n"
     ]
    }
   ],
   "source": [
    "def probabilities(attribute):\n",
    "    ''' \n",
    "    compute probabilities (used for computation of entropy)\n",
    "    '''\n",
    "    uniqueVals, val_counts=np.unique(attribute,return_counts=True)\n",
    "    val_counts = np.float64(val_counts)\n",
    "    val_probs = val_counts/np.float64(attribute.size)\n",
    "    return uniqueVals, val_probs\n",
    "def entropy(attribute):\n",
    "    ''' \n",
    "    compute entropy which is used for infogain\n",
    "    '''\n",
    "    uniqueVals , val_probs =probabilities(attribute)\n",
    "    return np.sum(val_probs *np.log2(1./val_probs))\n",
    "def infoGain(attribute,attributes):\n",
    "    ''' \n",
    "    compute infogain for a given attribute\n",
    "    '''\n",
    "    attr_probs = probabilities(attribute)\n",
    "    label_entropy = entropy(attributes)\n",
    "    label_given_attr = []\n",
    "    if type(attr_probs[1]) != np.ndarray:\n",
    "        return label_entropy\n",
    "    cts_dict = dict(zip(attr_probs[0],attr_probs[1]))\n",
    "    for uniq_attr_val,uniq_val_prob in cts_dict.items():\n",
    "        label_given_attr.append( uniq_val_prob *entropy(attributes[attribute==uniq_attr_val]))\n",
    "    return label_entropy- np.sum(label_given_attr)\n",
    "def find_best_attr(trainData, attribute_names,attributes):\n",
    "    ''' \n",
    "    Find best attribute to split data on\n",
    "    '''\n",
    "    gains=np.array([])\n",
    "    for i in trainData:\n",
    "        gains=np.append( gains, infoGain(i, attributes))\n",
    "    maxID = np.where(gains == np.max(gains))[0][0]\n",
    "    maxgn = gains[maxID]\n",
    "    att_name= attribute_names[maxID]\n",
    "    print('Max Gain:', maxgn)\n",
    "    return att_name,maxID\n",
    "def stringToInt(attribute):\n",
    "    ''' \n",
    "    Convert attribute from string to int\n",
    "    '''\n",
    "    uniqueVals, val_counts = np.unique(attribute,return_counts=True)\n",
    "    map_ints = range(len(uniqueVals))\n",
    "    map_attr = np.copy(attribute)\n",
    "    for i in range(len(uniqueVals)):\n",
    "        all_val = np.where(attribute == uniqueVals[i])[0]\n",
    "        map_attr[all_val] = map_ints[i]\n",
    "        intvals = map_ints[i]\n",
    "        allints = np.ones_like(all_val)*intvals\n",
    "    return [uniqueVals, map_ints, val_counts,map_attr]\n",
    "class DT:\n",
    "    def __init__(self, attr_split=None,parent=None):\n",
    "        self.parent=parent\n",
    "        self.child=[]\n",
    "        self.attr_split= attr_split\n",
    "    def add_child(self,child):\n",
    "        self.child.append(child)\n",
    "        child.parent= self\n",
    "def isNaN(arr1,arr2):\n",
    "    nans = np.where( (~np.isnan(arr1)) &(~np.isnan(arr2)))[0]\n",
    "    fin_arr1 = arr1[nans]\n",
    "    fin_arr2 = arr2[nans]\n",
    "    return fin_arr1,fin_arr2,nans\n",
    "def findBestSplit(attribute,attributes,depth=0):\n",
    "    ''' \n",
    "    find best splitting point based on attribute type (numeric or categorical)\n",
    "    '''\n",
    "    try:\n",
    "        attr_dat =np.float64(attribute)\n",
    "        uniqueVals,probs = probabilities(attribute)\n",
    "        typ='num'    \n",
    "    except:\n",
    "        uniqueVals,map_ints, val_counts, map_attr = stringToInt(attribute)\n",
    "        attr_dat = np.copy(np.float64(map_attr))\n",
    "        typ='cat'\n",
    "    if typ =='cat' or len(uniqueVals) <=10:\n",
    "        n_splits = len(uniqueVals)\n",
    "        splits =[]\n",
    "        split_ints = []\n",
    "        for i in range(n_splits):\n",
    "            splits.append([attribute==uniqueVals[i] ])\n",
    "            split_ints.append(uniqueVals[i])\n",
    "        threshold_val = np.array([-99])\n",
    "        threshold_type ='NA'\n",
    "    elif len(uniqueVals) >2:\n",
    "        try:\n",
    "            attributes = np.float64(attributes)\n",
    "            sort_by_attr = np.argsort(attr_dat)\n",
    "            attr_sort = np.copy(attr_dat[sort_by_attr])\n",
    "            label_sort =np.copy(attributes[sort_by_attr])\n",
    "            linearegslopes_left = []\n",
    "            linearegslopes_right = []\n",
    "            for i in range(len(attr_sort)//10,len(attr_sort)-len(attr_sort)//10):\n",
    "                attr_l = attr_sort[0:i]\n",
    "                attr_r = attr_sort[i:]\n",
    "                lab_l = label_sort[0:i]\n",
    "                lab_r = label_sort[i:]\n",
    "                m_l,b_l = np.polyfit(attr_l,lab_l,1)\n",
    "                m_r,b_r = np.polyfit(attr_r,lab_r,1)\n",
    "                linearegslopes_left.append(abs(m_l))\n",
    "                linearegslopes_right.append(abs(m_r))\n",
    "            linearegslopes_left =np.array(linearegslopes_left)\n",
    "            linearegslopes_right =np.array(linearegslopes_right)\n",
    "            finite_left,finite_right,finite_inds = isNaN(linearegslopes_left,linearegslopes_right)\n",
    "            inv_left = 1./finite_left\n",
    "            inv_right = 1./finite_right\n",
    "            linearleft=finite_left*inv_right\n",
    "            linright = inv_left*finite_right\n",
    "            finmxleft = np.where(linearleft == np.max(linearleft))[0][0]\n",
    "            mxleft = np.where(linearegslopes_left == finite_left[finmxleft])[0]\n",
    "            finmxright = np.where(linearight == np.max(linearight))[0][0]\n",
    "            mxright = np.where(linearegslopes_right == finite_right[finmxright])[0]\n",
    "            threshold_val = attr_sort[mxleft+len(attr_sort)//10]        \n",
    "            if typ=='cat':\n",
    "                split_l = [attr_dat <threshold_val]\n",
    "                split_r = [attr_dat >=threshold_val]\n",
    "                splits=[split_l,split_r]\n",
    "            else:\n",
    "                split_l = [attr_dat<=threshold_val]\n",
    "                split_r= [attr_dat >threshold_val]\n",
    "                splits=[split_l,split_r]\n",
    "            threshold_type='linearSplit'\n",
    "            split_ints =[]\n",
    "        except:\n",
    "            threshold_val = np.array([np.mean(attr_dat)])\n",
    "            split_l =[attr_dat < threshold_val]\n",
    "            split_r = [attr_dat >= threshold_val]\n",
    "            splits = [split_l,split_r]\n",
    "            threshold_type = 'linearSplit'\n",
    "            split_ints = []\n",
    "    return splits,threshold_val,threshold_type,split_ints,typ\n",
    "def pure(vals):\n",
    "    ''' \n",
    "    Check if vals are pure or not\n",
    "    '''\n",
    "    return len(np.unique(vals))==1\n",
    "def buildTree(dat,attribute_names,attributes,root=None,depth=0):\n",
    "    '''\n",
    "    find best attribute to split on, best splitting point and build a decision tree recursively\n",
    "    '''\n",
    "    if dat[0] ==[]:\n",
    "        root.avg_label = root.parent.avg_label\n",
    "        return\n",
    "    if len(dat[0]) <5:\n",
    "        root.avg_label= np.mean(attributes)\n",
    "        return\n",
    "    bestattr,maxID= find_best_attr(dat,attribute_names,attributes)\n",
    "    if not root:\n",
    "        root = DT()\n",
    "    root.split_attr =bestattr\n",
    "    root.split_attr_ind=maxID\n",
    "    attr_data = np.copy(dat[maxID])\n",
    "    if pure(attr_data):\n",
    "        root.avg_label= np.mean(attributes)\n",
    "        return \n",
    "    splits,threshold_val, threshold_type,split_ints,typ=findBestSplit(attr_data,attributes,depth=depth)\n",
    "    trees = []\n",
    "    for i in range(len(splits)):\n",
    "        trees.append(DT(parent=root))\n",
    "        root.add_child(trees[i])\n",
    "        root.split_ints = split_ints\n",
    "        trees[i].inds_filt = splits[i][0]\n",
    "        \n",
    "    root.threshold_val =threshold_val\n",
    "    root.threshold_type = threshold_type\n",
    "    root.typ = typ\n",
    "    root.avg_label= np.mean(attributes)\n",
    "    depth+=1\n",
    "    red= [attribute_names!=bestattr]\n",
    "\n",
    "    red_attr_names = attribute_names[red]\n",
    "    red_data = dat[red]\n",
    "    for tree in trees:\n",
    "        buildTree(red_data[:,tree.inds_filt],red_attr_names,attributes[tree.inds_filt],root=tree,depth=depth)\n",
    "    return root \n",
    "price_by_id = {}\n",
    "decisionT = buildTree(trainData,uniqueLabels, housingPrices)\n",
    "\n",
    "def predict(dat,uniqueLabels,dTree,predTree = None,attributes=[]):\n",
    "    ''' \n",
    "    Predict target attributes (housing prices)\n",
    "    '''\n",
    "    if len(dat[0]) == 0:\n",
    "        return\n",
    "    if len(attributes) == 0:\n",
    "        attributes = np.ones_like(dat[0])\n",
    "    if len(dTree.child) ==0:\n",
    "        for tid in dat[0]:\n",
    "            price_by_id[tid] = dTree.avg_label \n",
    "        return\n",
    "    if not predTree:\n",
    "        predTree = DT()\n",
    "    attr_split = dTree.split_attr \n",
    "    attr_split_ind =np.where(uniqueLabels == attr_split)[0]\n",
    "    attr_split_ints = dTree.split_ints\n",
    "    attr_dat = dat[attr_split_ind][0]\n",
    "    thresh_type = dTree.threshold_type\n",
    "    thresh_val = np.float64(dTree.threshold_val[0])\n",
    "    try:\n",
    "        attr_data =np.float64(attr_dat)\n",
    "        uniqueVals,probs = probabilities(attr_dat)\n",
    "    except:\n",
    "        uniqueVals,map_ints, val_counts, map_attr = stringToInt(attr_dat)\n",
    "        attr_data = np.copy(np.float64(map_attr))\n",
    "    typ = dTree.typ\n",
    "    predTree.attr_split = attr_split\n",
    "    if thresh_type=='0':\n",
    "        split_l = np.where(attr_data == 0)[0]\n",
    "        split_r = np.where(attr_data > 0)[0]\n",
    "        splits = [split_l,split_r]\n",
    "    elif thresh_type=='linearSplit':\n",
    "        if typ=='cat':\n",
    "            split_l = np.where(attr_data <thresh_val)[0]\n",
    "            split_r = np.where(attr_data >=thresh_val)[0]\n",
    "            splits=[split_l,split_r]\n",
    "        else:\n",
    "            split_l = np.where(attr_data<=thresh_val)[0]\n",
    "            split_r= np.where(attr_data >thresh_val)[0]       \n",
    "            splits=[split_l,split_r]   \n",
    "    elif thresh_type == 'NA':\n",
    "        splits =[]\n",
    "        for val in attr_split_ints:\n",
    "            splits.append(np.where(attr_data==val)[0])    \n",
    "    trees = []\n",
    "    for i in range(len(splits)):\n",
    "        trees.append(DT(parent=predTree))\n",
    "        predTree.add_child(trees[i])\n",
    "        trees[i].label_inds = splits[i]\n",
    "        trees[i].dtree = dTree.child[i]\n",
    "        trees[i].avg_label =dTree.avg_label\n",
    "        predict(dat[:,trees[i].label_inds],uniqueLabels,trees[i].dtree, predTree= trees[i],attributes=attributes)\n",
    "    tids = dat[0]\n",
    "    labs =[]\n",
    "    for tid in tids:\n",
    "        try:\n",
    "            labs.append(price_by_id[tid])\n",
    "        except:\n",
    "            labs.append(np.median(list(price_by_id.values() )))\n",
    "    return labs\n",
    "p = predict(testData, testAttributes,decisionT)\n",
    "\n",
    "def __mean_squared_error():\n",
    "        \"\"\"\n",
    "        Calculates mean squared error for a selection of records.\n",
    "\n",
    "        :param records: Data records (given by indices)\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        global p\n",
    "        global testTarget\n",
    "        SQE = 0\n",
    "        testTargetFloat = [float(i) for i in testTarget]\n",
    "        for i,x in enumerate(p):\n",
    "            SQE = SQE + ((10**x)-float(testTarget[i]))**2\n",
    "        MSE = np.sqrt(SQE/len(p))/np.std(testTargetFloat)\n",
    "        return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def createSubmission(test_ids, predictions):\n",
    "    sub = pd.DataFrame()\n",
    "    sub['Id'] = test_ids\n",
    "    sub['SalePrice'] = predictions\n",
    "    sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the main function building a decision tree model, printing it and applying it on some unseen records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    test_ids = []\n",
    "    predictions = []\n",
    "    for i in range(len(testData[0])):\n",
    "        test_ids.append(str(testData[0][i]))\n",
    "        predictions.append(str(10**p[i]))\n",
    "    \n",
    "    createSubmission(test_ids, predictions)\n",
    "    \n",
    "    #https://www.kaggle.com/c/house-prices-advanced-regression-techniques/leaderboard\n",
    "    print(\"Kaggle Test Score: 0.29785\")\n",
    "    \n",
    "    #print(\"Root-Mean-Square-Error (normalized using np.std):\")\n",
    "    #print(__mean_squared_error())\n",
    "\n",
    "    \n",
    "    #print_tree(decision_tree)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
