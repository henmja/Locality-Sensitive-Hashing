{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Building a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a skeleton of a decision tree classifier for the example data set in `data/example.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "from statistics import median, mode, mean\n",
    "from collections import Counter\n",
<<<<<<< HEAD
    "from enum import Enum\n",
    "import numpy as np\n",
    "import scipy.stats as st\n"
=======
    "from  enum import Enum\n",
    "import  numpy as np\n",
    "\n",
    "#A = [[1,2,3],[1,2,3],[1,2,3]]\n",
    "\n",
    "#test = set([A[idx][1] for idx in range(len(A))])\n",
    "#print(test)\n",
    "#print(A[:,0])\n",
    "#test2 = {}\n",
    "#test2[1] = []\n",
    "#temp = np.where( A[:,0] <= 3 )\n",
    "#print(temp[0])\n",
    "#def test(x):\n",
    "#    print(x)\n",
    "#    for i in range(0,2):\n",
    "#        test(i)\n",
    "        \n",
    "#test('test')\n",
    "    \n",
    "#test2[1].append(temp[0])\n",
    "#listi = np.array(test2[1][0].tolist())\n",
    "#print(listi)\n",
    "#print(listi[0])\n",
    "#l = [idx for idx in listi]\n",
    "#print(l)\n",
    "\n",
    "#ls = [1,1,1,2,2,2,3,4,5,5,8,8]\n",
    "#print([ls.count(ls[i]) for i in range(len(ls))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some simple type definitions."
   ]
  },
  {
   "cell_type": "code",
   
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrType(Enum):\n",
    "    cat = 0  # categorical (qualitative) attribute\n",
    "    num = 1  # numerical (quantitative) attribute\n",
    "    target = 2  # target label\n",
    "\n",
    "\n",
    "class NodeType(Enum):\n",
    "    root = 0\n",
    "    internal = 1\n",
    "    leaf = 2\n",
    "\n",
    "\n",
    "class SplitType(Enum):\n",
    "    bin = 0  # binary split\n",
    "    multi = 1  # multi-way split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, some basic classes to represent an attribute, a spltting procedure, and a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attribute(object):\n",
    "    def __init__(self, label, type):\n",
    "        assert type in AttrType\n",
    "        self.label = label\n",
    "        self.type = type\n",
    "        self.stat = None  # holds mean for numerical and mode for categorical attributes\n",
    "\n",
    "\n",
    "class Splitting(object):\n",
    "    def __init__(self, attr, infogain, split_type, cond, splits):\n",
    "        self.attr = attr  # attribute ID (index in ATTR)\n",
    "        self.infogain = infogain  # information gain if splitting is done on this attribute\n",
    "        self.split_type = split_type  # one of SplitType\n",
    "        self.cond = cond  # splitting condition, i.e., values on outgoing edges\n",
    "        self.splits = splits  # list of training records (IDs) for each slitting condition\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, id, type, parent_id, children=None, edge_value=None, val=None, split_type=None, split_cond=None,\n",
    "                 infogain=None):\n",
    "        self.id = id  # ID (same as the index in DT.model list)\n",
    "        self.type = type  # one of NodeType\n",
    "        self.parent_id = parent_id  # ID of parent node (None if root)\n",
    "        self.children = children  # list of IDs of child nodes\n",
    "        self.edge_value = edge_value  # the value of the incoming edge (only if not root node)\n",
    "        self.val = val  # if root or internal node: the attribute that is compared at that node; if leaf node: \n",
    "        #the target value\n",
    "        self.split_type = split_type  # one of SplitType\n",
    "        self.split_cond = split_cond  # splitting condition (median value for binary splits on numerical values; otherwise a list of categorical values (corresponding to child nodes))\n",
    "        self.infogain = infogain\n",
    "\n",
    "    def append_child(self, node_id):\n",
    "        self.children.append(node_id)"
>>>>>>> 2ec84d4e2537fd03958a72b1b864c4c324d06040
   ]
  },
  {
   "cell_type": "markdown",
   
   "metadata": {},
   "source": [
    "The input filename is hard-coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet = np.loadtxt('data/housing_price_train.csv',dtype='<U20',delimiter=',')\n",
    "testSet = np.loadtxt('data/housing_price_test.csv',dtype='<U20',delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute labels types are hard-coded too (the same order as in the file!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "test_attributes = testSet[0]\n",
    "trainingTarget= np.transpose(np.copy(trainingSet[1:]))\n",
    "testData = np.transpose(np.copy(testSet[1:]))\n",
    "testData = np.copy(testData)\n",
    "test_attr_names =np.copy(test_attributes)"
=======
    "#Read from training data:\n",
    "with open(INFILE) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file,  delimiter=',')\n",
    "    row1 = next(csv_reader)\n",
    "    labels = []\n",
    "    types = []\n",
    "    ATTR = []\n",
    "    i = 0\n",
    "    for x in row1:\n",
    "        if i>0: #skip first element (ID)\n",
    "            labels.append(x)\n",
    "        i=i+1\n",
    "    \n",
    "    types = [AttrType.num, AttrType.cat, AttrType.num, AttrType.num, AttrType.cat, AttrType.cat, AttrType.cat,\n",
    "            AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat,\n",
    "            AttrType.cat, AttrType.num, AttrType.num, AttrType.num, AttrType.num, AttrType.cat, AttrType.cat, AttrType.cat,\n",
    "            AttrType.cat, AttrType.cat, AttrType.num, AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat, AttrType.cat,\n",
    "            AttrType.cat, AttrType.cat, AttrType.num, AttrType.cat, AttrType.num, AttrType.num, AttrType.num, AttrType.cat,\n",
    "            AttrType.cat, AttrType.cat, AttrType.cat, AttrType.num, AttrType.num, AttrType.num, AttrType.num, AttrType.num,\n",
    "            AttrType.num, AttrType.num, AttrType.num, AttrType.num, AttrType.num, AttrType.cat, AttrType.num, AttrType.cat,\n",
    "            AttrType.num, AttrType.cat, AttrType.cat, AttrType.num, AttrType.cat, AttrType.num, AttrType.num, AttrType.cat,\n",
    "            AttrType.cat, AttrType.cat, AttrType.num, AttrType.num, AttrType.num, AttrType.num, AttrType.num, AttrType.num,\n",
    "            AttrType.cat, AttrType.cat, AttrType.cat, AttrType.num, AttrType.num, AttrType.num, AttrType.cat, AttrType.cat,\n",
    "            AttrType.target]\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        ATTR.append(Attribute(labels[i],types[i]))\n",
    "        \n",
    "    print(len(labels))\n",
    "    print(len(ATTR))"
>>>>>>> 2ec84d4e2537fd03958a72b1b864c4c324d06040
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index of the target attribute (assuming it's the last)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_TARGET = np.log10(np.float64(trainingTarget[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A main class DT representing the decision tree classifier. It could represent with methods:\n",
    "\n",
    "  - a given impurity measure;\n",
    "  - the search for the best attribute to split with;\n",
    "  - the addition of a node to the tree;\n",
    "  - a convenient model printer;\n",
    "  - the recursive call for obtaining a tree;\n",
    "  - a builder and an applier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henri\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:185: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Gain: 8.045796138342293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henri\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:215: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "C:\\Users\\henri\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:216: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Gain: 7.446101489756081\n",
      "Max Gain: 6.524561605715065\n",
      "Max Gain: 6.497662618424865\n",
      "Max Gain: 5.65645677569764\n",
      "Max Gain: 5.631615665225583\n",
      "Max Gain: 5.8574319218048085\n",
      "Max Gain: 6.595891163108825\n",
      "Max Gain: 7.149981774453494\n",
      "Max Gain: 7.43752996644357\n",
      "Max Gain: 7.331770310806017\n",
      "Max Gain: 7.21647843135994\n",
      "Max Gain: 7.813184524595746\n",
      "Max Gain: 7.126460214904276\n",
      "Max Gain: 6.247495789386386\n",
      "Max Gain: 5.94770277922009\n",
      "Max Gain: 6.058984089445426\n",
      "Max Gain: 6.87496155606346\n",
      "Max Gain: 7.164965687925738\n",
      "Max Gain: 6.457972024550777\n",
      "Max Gain: 6.11249200111032\n",
      "Max Gain: 5.304682449772211\n",
      "Max Gain: 4.566108939837479\n",
      "Max Gain: 4.436605434317882\n",
      "Max Gain: 3.321928094887362\n",
      "Max Gain: 3.7004397181410926\n",
      "Max Gain: 6.364489555653822\n",
      "Max Gain: 5.720049960644811\n",
      "Max Gain: 5.155399311574898\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "def stringToInt(attribute):\n",
    "    ''' \n",
    "    Convert numeric attributes from string to int\n",
    "    '''\n",
    "    uniq_vals, val_counts = np.unique(attribute,return_counts=True)\n",
    "    map_ints = range(len(uniq_vals))\n",
    "    map_attr = np.copy(attribute)\n",
    "    for i in range(len(uniq_vals)):\n",
    "        all_val = np.where(attribute == uniq_vals[i])[0]\n",
    "        map_attr[all_val] = map_ints[i]\n",
    "        intvals = map_ints[i]\n",
    "        allints = np.ones_like(all_val)*intvals\n",
    "    return [uniq_vals, map_ints, val_counts,map_attr]\n",
    "def probabilities(attribute):\n",
    "    ''' \n",
    "    Probabilities for entropy calculation\n",
    "    '''\n",
    "    uniq_vals, val_counts=np.unique(attribute,return_counts=True)\n",
    "    val_counts = np.float64(val_counts)\n",
    "    val_probs = val_counts/np.float64(attribute.size)\n",
    "    return uniq_vals, val_probs\n",
    "def entropy(attribute):\n",
    "    ''' \n",
    "    Compute entropy, which is used to compute infogain\n",
    "    '''\n",
    "    uniq_vals , val_probs =probabilities(attribute)\n",
    "    return np.sum(val_probs *np.log2(1./val_probs))\n",
    "def infogain(attribute,labels):\n",
    "    ''' \n",
    "    Compute infogains\n",
    "    '''\n",
    "    cts_arr =probabilities(attribute)\n",
    "    label_entropy = entropy(labels)\n",
    "    label_given_attr = []\n",
    "    if type(cts_arr[1]) != np.ndarray:\n",
    "        return label_entropy\n",
    "    cts_dict = dict(zip(cts_arr[0],cts_arr[1]))\n",
    "    for uniq_attr_val,uniq_val_prob in cts_dict.items():\n",
    "        label_given_attr.append( uniq_val_prob *entropy(labels[attribute==uniq_attr_val]))\n",
    "    return label_entropy- np.sum(label_given_attr)\n",
    "def bestAttr(data, attribute_names,labels):\n",
    "    ''' \n",
    "    the best attribute is the one that gives maximum infogain\n",
    "    '''\n",
    "    gains=np.array([])\n",
    "    for i in data:\n",
    "        gains=np.append( gains, infogain(i, labels))\n",
    "    maxind = np.where(gains == np.max(gains))[0][0]\n",
    "    maxgn = gains[maxind]\n",
    "    att_name= attribute_names[maxind]\n",
    "    print('Max Gain:', maxgn)\n",
    "    return att_name,maxind\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DT:\n",
    "    def __init__(self, attr_split=None,parent=None):\n",
    "        self.attr_split= attr_split\n",
    "        self.parent=parent\n",
    "        self.child=[]\n",
    "        \n",
    "    def add_child(self,child):\n",
    "        ''' \n",
    "        Add child node\n",
    "        '''\n",
    "        self.child.append(child)\n",
    "        child.parent= self\n",
    "        \n",
=======
    "class DT(object):\n",
    
    "    def __init__(self):\n",
    "        self.data = None  # training data set (loaded into memory)\n",
    "        self.model = None  # decision tree model\n",
    "        self.default_target = 0.0  # default target class\n",
    "\n",
    "    def __load_data(self):\n",
    "        with open(INFILE) as csvfile:\n",
    "            self.data = []\n",
    "            csvreader = csv.reader(csvfile, delimiter=',')\n",
    "            next(csvreader) #REMOVE FIRST ROW\n",
    "            for row in csvreader:\n",
    "                iterRow = iter(row)\n",
    "                next(iterRow) #remove first column (IDS)\n",
    "                row = list(iterRow)\n",
    "                rec = []\n",
    "                for i in range(len(ATTR)):\n",
    "                    val = row[i].strip()\n",
    "                    #print(val)\n",
    "                    # convert numerical attributes\n",
    "                    if val == 'NA':\n",
    "                        val = -1\n",
    "                    if ATTR[i].type == AttrType.num:  # Note that this will break for \"?\" (missing attribute)\n",
    "                            val = float(val)\n",
    "                    rec.append(val)\n",
    "                self.data.append(rec)\n",
    "                #self.data.pop(0) #REMOVE FIRST COLUMN (IDS)\n",
    "                #for row in self.data:\n",
    "                #    del row[0] #REMOVE FIRST ROW (LABELS)\n",
    "                #print(self.data[0])\n",
    "                # self.data.append([element.strip() for element in row])  # strip spaces\n",
    "\n",
    "\n",
    "    \n",
>>>>>>> 2ec84d4e2537fd03958a72b1b864c4c324d06040
    "    def __mean_squared_error(self, records):\n",
    "        \"\"\"\n",
    "        Calculates mean squared error for a selection of records.\n",
    "\n",
    "        :param records: Data records (given by indices)\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        predictions = self.predict(records)\n",
    "        sqe = np.zeros(len(records)) #square error\n",
    "        i = 0\n",
    "        for rec in records:\n",
    "            sqe[i] = (self.data[records,IDX_TARGET]-predictions[records, IDX_TARGET])**2 \n",
    "            i = i+1\n",
    "        MSE = mean(sqe) #mean square error\n",
    "        return MSE\n",
    "    \n",
    "    \n",
    "def pureAttr(vals):\n",
    "    ''' \n",
    "    Attribute with a purity of 1\n",
    "    '''\n",
    "    return len(np.unique(vals))==1\n",
    "\n",
    "def isFinite(arr1,arr2):\n",
    "    ''' \n",
    "    Check if input arrays consist of finite numbers\n",
    "    '''\n",
    "    finds = np.where((~np.isnan(arr1)) &(~np.isnan(arr2)))[0]\n",
    "    fin_arr1 = arr1[finds]\n",
    "    fin_arr2 = arr2[finds]\n",
    "    return fin_arr1,fin_arr2,finds\n",
    "\n",
    "def split(attr,labels,attr_name,depth=0):\n",
    "    ''' \n",
    "    Split attribute data\n",
    "    '''\n",
    "    try:\n",
    "        attr_dat =np.float64(attr)\n",
    "        uniq_vals,probs = probabilities(attr)\n",
    "        typ='num'    \n",
    "    except:\n",
    "        uniq_vals,map_ints, val_counts, map_attr = stringToInt(attr)\n",
    "        attr_dat = np.copy(np.float64(map_attr))\n",
    "        typ='nom'\n",
    "        print('Map Attr',map_attr)\n",
    "    if typ =='nom' or len(uniq_vals) <=10:\n",
    "        n_splits = len(uniq_vals)\n",
    "        splits =[]\n",
    "        split_ints = []\n",
    "        for i in range(n_splits):\n",
    "            splits.append([attr==uniq_vals[i] ])\n",
    "            split_ints.append(uniq_vals[i])\n",
    "        threshold_val = np.array([-99])\n",
    "        threshold_type ='NA'\n",
    "    elif len(uniq_vals) >2:\n",
    "        try:\n",
    "            labels = np.float64(labels)\n",
    "            sort_by_attr = np.argsort(attr_dat)\n",
    "            attr_sort = np.copy(attr_dat[sort_by_attr])\n",
    "            label_sort =np.copy(labels[sort_by_attr])\n",
    "            linearRegslopes_left = []\n",
    "            linearRegslopes_right = []\n",
    "            for i in range(len(attr_sort)//10,len(attr_sort) -len(attr_sort)//10  ):\n",
    "                attr_l = attr_sort[0:i]\n",
    "                attr_r = attr_sort[i:]\n",
    "                lab_l = label_sort[0:i]\n",
    "                lab_r = label_sort[i:]\n",
    "                m_l,b_l = np.polyfit(attr_l,lab_l,1)\n",
    "                m_r,b_r = np.polyfit(attr_r,lab_r,1)\n",
    "                linearRegslopes_left.append(abs(m_l))\n",
    "                linearRegslopes_right.append(abs(m_r))\n",
    "            linearRegslopes_left =np.array(linearRegslopes_left)\n",
    "            linearRegslopes_right =np.array(linearRegslopes_right)\n",
    "            finite_left,finite_right,finite_inds = is_finite(linearRegslopes_left,linearRegslopes_right)\n",
    "            inv_left = 1./finite_left\n",
    "            inv_right = 1./finite_right\n",
    "            linearLeft=finite_left*inv_right\n",
    "            linearRight = inv_left*finite_right\n",
    "            finmxleft = np.where(linearLeft == np.max(linearLeft))[0][0]\n",
    "            mxleft = np.where(linearRegslopes_left == finite_left[finmxleft])[0]\n",
    "            finmxright = np.where(linearRight == np.max(linearRight))[0][0]\n",
    "            mxright = np.where(linearRegslopes_right == finite_right[finmxright])[0]\n",
    "            threshold_val = attr_sort[mxleft+len(attr_sort)//10]\n",
    "                   \n",
    "            if typ=='nom':\n",
    "                split_l = [attr_dat <threshold_val]\n",
    "                split_r = [attr_dat >=threshold_val]\n",
    "                splits=[split_l,split_r]\n",
    "            else:\n",
    "                split_l = [attr_dat<=threshold_val]\n",
    "                split_r= [attr_dat >threshold_val]\n",
    "                splits=[split_l,split_r]\n",
    "            threshold_type='linsplit'\n",
    "            split_ints =[]\n",
    "        except:\n",
    "            threshold_val = np.array([np.mean(attr_dat)])\n",
    "            split_l =[attr_dat < threshold_val]\n",
    "            split_r = [attr_dat >= threshold_val]\n",
    "            splits = [split_l,split_r]\n",
    "            threshold_type = 'linsplit'\n",
    "            split_ints = []\n",
    "    return splits,threshold_val,threshold_type,split_ints,typ\n",
    "\n",
    "def print_tree(tree):\n",
    "    '''\n",
    "    Print split info\n",
    "    '''\n",
    "    print(tree.split_attr)\n",
    "    print(tree.threshold_val)\n",
    "    print(tree.avg_label)\n",
    "\n",
    "def buildTree(dat,attribute_names,labels,root=None,depth=0):\n",
    "    '''\n",
    "    Find best attribute to split on, the values for each splits and build a decision tree by calling this function recursively.\n",
    "    '''\n",
    "    if dat[0] ==[]:\n",
    "        root.avg_label = root.parent.avg_label\n",
    "        return \n",
    "    if len(dat[0]) <5:\n",
    "        root.avg_label= np.mean(labels)\n",
    "        return\n",
    "    bestattr,maxind= bestAttr(dat,attribute_names,labels)\n",
    "    if not root:\n",
    "        root = DT()\n",
    "    root.split_attr =bestattr\n",
    "    root.split_attr_ind=maxind\n",
    "    attr_data = np.copy(dat[maxind])\n",
    "    if pureAttr(attr_data):\n",
    "        root.avg_label= np.mean(labels)\n",
    "        return \n",
    "    splits,threshold_val, threshold_type,split_ints,typ=split(attr_data,labels,bestattr,depth=depth)\n",
    "    trees = []\n",
    "    for i in range(len(splits)):\n",
    "        trees.append(Tree(parent=root))\n",
    "        root.add_child(trees[i])\n",
    "        root.split_ints = split_ints\n",
    "        trees[i].inds_filt = splits[i][0]\n",
    "        \n",
    "    root.threshold_val =threshold_val\n",
    "    root.threshold_type = threshold_type\n",
    "    root.typ = typ\n",
    "    root.avg_label= np.mean(labels)\n",
    "    depth+=1\n",
    "    red= [attribute_names!=bestattr]\n",
    "\n",
    "    red_attr_names = attribute_names[red]\n",
    "    red_data = dat[red]\n",
    "    for tree in trees:\n",
    "        buildTree(red_data[:,tree.inds_filt],red_attr_names,labels[tree.inds_filt],root=tree,depth=depth)\n",
    "    return root \n",
    "targetId = {}\n",
    "tree = buildTree(data,attr_names, IDX_TARGET)\n",
    "\n",
    "\n",
    "def predictlabels(dat,attr_names,dTree,predTree = None,labels=[]):\n",
    "    '''\n",
    "    Predict target/class values\n",
    "    '''\n",
    "    if len(dat[0]) == 0:\n",
    "        return\n",
    "    if len(labels) == 0:\n",
    "        labels = np.ones_like(dat[0])\n",
    "    if len(dTree.child) ==0:\n",
    "        for tid in dat[0]:\n",
    "            targetId[tid] = dTree.avg_label \n",
    "        return\n",
    "    if not predTree:\n",
    "        predTree = Tree()\n",
    "    attr_split = dTree.split_attr \n",
    "    attr_split_ind =np.where(attr_names == attr_split)[0]\n",
    "    attr_split_ints = dTree.split_ints\n",
    "    attr_dat = dat[attr_split_ind][0] \n",
    "    thresh_type = dTree.threshold_type\n",
    "    thresh_val = np.float64(dTree.threshold_val[0])\n",
    "    try:\n",
    "        attr_data =np.float64(attr_dat)\n",
    "        uniq_vals,probs = probabilities(attr_dat)\n",
    "    except:\n",
    "        uniq_vals,map_ints, val_counts, map_attr = stringToInt(attr_dat)\n",
    "        attr_data = np.copy(np.float64(map_attr))\n",
    "    typ = dTree.typ\n",
    "    predTree.attr_split = attr_split\n",
    "    if thresh_type=='0':\n",
    "        split_l = np.where(attr_data == 0)[0]\n",
    "        split_r = np.where(attr_data > 0)[0]\n",
    "        splits = [split_l,split_r]\n",
    "    elif thresh_type=='linsplit':\n",
    "        if typ=='nom':\n",
    "            split_l = np.where(attr_data <thresh_val)[0]\n",
    "            split_r = np.where(attr_data >=thresh_val)[0]\n",
    "            splits=[split_l,split_r]\n",
    "        else:\n",
    "            split_l = np.where(attr_data<=thresh_val)[0]\n",
    "            split_r= np.where(attr_data >thresh_val)[0]       \n",
    "            splits=[split_l,split_r]   \n",
    "    elif thresh_type == 'NA':\n",
    "        splits =[]\n",
    "        for val in attr_split_ints:\n",
    "            splits.append(np.where(attr_data==val)[0])    \n",
    "    trees = []\n",
    "    for i in range(len(splits)):\n",
    "        trees.append(Tree(parent=predTree))\n",
    "        predTree.add_child(trees[i])\n",
    "        trees[i].label_inds = splits[i]\n",
    "        trees[i].dtree = dTree.child[i]\n",
    "        trees[i].avg_label =dTree.avg_label\n",
    "        predictlabels(dat[:,trees[i].label_inds],attr_names,trees[i].dtree, predTree= trees[i],labels=labels)\n",
    "    tids = dat[0]\n",
    "    labs =[]\n",
    "    for tid in tids:\n",
    "        try:\n",
    "            labs.append(targetId[tid])\n",
    "        except:\n",
    "            labs.append(np.median(list(targetId.values() )))\n",
    "    return labs\n",
    "p = predictlabels(testData, test_attr_names,tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def createSubmission(test_ids, predictions):\n",
    "    sub =   pd.DataFrame()\n",
    "    sub['Id'] = test_ids\n",
    "    sub['SalePrice'] = predictions\n",
    "    sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the main function building a decision tree model, printing it and applying it on some unseen records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LotArea\n",
      "[13860.]\n",
      "5.221978956475628\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    test_ids = []\n",
    "    predictions = []\n",
    "    for i in range(len(testData[0])):\n",
    "        test_ids.append(str(testData[0][i]))\n",
    "        predictions.append(str(10**p[i]))\n",
    "    \n",
    "    createSubmission(test_ids, predictions)\n",
    "    \n",
    "    print_tree(dT)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
